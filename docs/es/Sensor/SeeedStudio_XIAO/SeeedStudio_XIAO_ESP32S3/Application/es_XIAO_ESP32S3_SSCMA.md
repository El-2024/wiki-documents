---
description: Gu√≠a sobre c√≥mo ir desde conjuntos de datos propios, etiquetado, entrenamiento y despliegue a XIAO ESP32S3.
title: Desplegando Modelos desde Conjuntos de Datos a XIAO ESP32S3
keywords:
- SSCMA
- xiao
image: https://files.seeedstudio.com/wiki/seeed_logo/logo_2023.png
slug: /es/xiao_esp32s3_sscma
last_update:
  date: 03/12/2024
  author: Citric
---


# Desplegando Modelos desde Conjuntos de Datos a XIAO ESP32S3

Bienvenido a este tutorial integral donde nos embarcaremos en un viaje para convertir tu conjunto de datos en un modelo completamente funcional para despliegue en el XIAO ESP32S3. En esta gu√≠a, navegaremos a trav√©s de los pasos iniciales de etiquetado de nuestro conjunto de datos con las herramientas intuitivas de Roboflow, progresando al entrenamiento del modelo dentro del entorno colaborativo de Google Colab.

Luego procederemos al despliegue de nuestro modelo entrenado usando el SenseCraft Model Assistant, un proceso que cierra la brecha entre el entrenamiento y la aplicaci√≥n en el mundo real. Al final de este tutorial, no solo tendr√°s un modelo personalizado ejecut√°ndose en XIAO ESP32S3, sino que tambi√©n estar√°s equipado con el conocimiento para interpretar y utilizar los resultados de las predicciones de tu modelo.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/0.png" style={{width:1000, height:'auto'}}/></div>


Desde el conjunto de datos hasta el despliegue del modelo, tendremos los siguientes pasos principales.

1. [Conjuntos de Datos Etiquetados](#labelled-datasets) ‚Äî‚Äî Este cap√≠tulo se enfoca en c√≥mo obtener conjuntos de datos que puedan ser entrenados en modelos. Hay dos formas principales de hacer esto. La primera es usar los conjuntos de datos etiquetados proporcionados por la comunidad de Roboflow, y la otra es usar tus propias im√°genes espec√≠ficas del escenario como conjuntos de datos, pero necesitas realizar manualmente el etiquetado t√∫ mismo.

2. [Modelo Exportado del Entrenamiento del Conjunto de Datos](#training-dataset-exported-model) ‚Äî‚Äî Este cap√≠tulo se enfoca en c√≥mo entrenar para obtener un modelo que pueda ser desplegado en XIAO ESP32S3 basado en el conjunto de datos obtenido en el primer paso, utilizando la plataforma Google Colab.

3. [Subir modelos v√≠a SenseCraft Model Assistant](#upload-models-via-sensecraft-model-assistant) ‚Äî‚Äî Esta secci√≥n describe c√≥mo usar el archivo de modelo exportado para subir el modelo a XIAO ESP32S3 usando el SenseCraft Model Assistant.

4. [Protocolos comunes y aplicaciones del modelo](#common-protocols-and-applications-of-the-model) ‚Äî‚Äî Finalmente, introduciremos el formato unificado de comunicaci√≥n de datos de SenseCraft AI para que puedas utilizar el m√°ximo potencial de tus dispositivos y modelos para crear aplicaciones que se ajusten a tus escenarios.

As√≠ que sumerj√°monos y comencemos el emocionante proceso de dar vida a tus datos.

## Materiales Requeridos

Antes de comenzar, es posible que necesites preparar el siguiente equipo.

<div class="table-center">
	<table align="center">
		<tr>
			<th>Seeed Studio XIAO ESP32S3</th>
			<th>Seeed Studio XIAO ESP32S3 Sense</th>
		</tr>
		<tr>
			<td><div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/SeeedStudio-XIAO-ESP32S3/img/xiaoesp32s3.jpg" style={{width:250, height:'auto'}}/></div></td>
			<td><div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/SeeedStudio-XIAO-ESP32S3/img/xiaoesp32s3sense.jpg" style={{width:250, height:'auto'}}/></div></td>
		</tr>
		<tr>
			<td><div class="get_one_now_container" style={{textAlign: 'center'}}>
				<a class="get_one_now_item" href="https://www.seeedstudio.com/XIAO-ESP32S3-p-5627.html" target="_blank">
				<strong><span><font color={'FFFFFF'} size={"4"}> Obtener Uno Ahora üñ±Ô∏è</font></span></strong>
				</a>
			</div></td>
			<td><div class="get_one_now_container" style={{textAlign: 'center'}}>
				<a class="get_one_now_item" href="https://www.seeedstudio.com/XIAO-ESP32S3-Sense-p-5639.html" target="_blank">
				<strong><span><font color={'FFFFFF'} size={"4"}> Obtener Uno Ahora üñ±Ô∏è</font></span></strong>
				</a>
			</div></td>
		</tr>
	</table>
</div>

Tanto las versiones XIAO ESP32S3 como Sense pueden utilizarse como contenido para este tutorial, pero dado que la versi√≥n est√°ndar del producto no permite el uso de la placa de expansi√≥n de c√°mara, recomendar√≠amos que uses la versi√≥n Sense.


## Conjuntos de Datos Etiquetados

En el contenido de esta secci√≥n, permitimos a los usuarios elegir libremente los conjuntos de datos que tengan. Esto incluye fotos de la comunidad o propias de la escena. Este tutorial introducir√° los dos escenarios dominantes. El primero es usar conjuntos de datos etiquetados ya preparados proporcionados por la comunidad de [Roboflow](https://roboflow.com/about). El otro es usar im√°genes de alta resoluci√≥n que hayas tomado y etiquetado el conjunto de datos. Por favor, lee los diferentes tutoriales a continuaci√≥n seg√∫n tus necesidades.

### Paso 1: Crear una cuenta gratuita de Roboflow

Roboflow proporciona todo lo que necesitas para etiquetar, entrenar y desplegar soluciones de visi√≥n por computadora. Para comenzar, crea una [cuenta gratuita de Roboflow](https://app.roboflow.com/?ref=blog.roboflow.com).

Despu√©s de revisar y aceptar los t√©rminos de servicio, se te pedir√° que elijas entre uno de dos planes: el Plan P√∫blico y el Plan Inicial.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/1.png" style={{width:800, height:'auto'}}/></div>


Luego, se te pedir√° que invites colaboradores a tu espacio de trabajo. Estos colaboradores pueden ayudarte a anotar im√°genes o gestionar los proyectos de visi√≥n en tu espacio de trabajo. Una vez que hayas invitado personas a tu espacio de trabajo (si quieres hacerlo), podr√°s crear un proyecto.


### Elige c√≥mo obtener tu conjunto de datos


import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
<TabItem value="Download Labelled datasets using Roboflow" label="Descargar conjuntos de datos etiquetados usando Roboflow">

Elegir un conjunto de datos adecuado de Roboflow para uso directo implica determinar el conjunto de datos que mejor se ajuste a los requisitos de tu proyecto, considerando aspectos como el tama√±o del conjunto de datos, calidad, relevancia y licencia.

**Paso 2. Explorar Roboflow Universe**

Roboflow Universe es una plataforma donde puedes encontrar varios conjuntos de datos. Visita el sitio web de Roboflow Universe y explora los conjuntos de datos disponibles.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/2.png" style={{width:1000, height:'auto'}}/></div>

Roboflow proporciona filtros y una funci√≥n de b√∫squeda para ayudarte a encontrar conjuntos de datos. Puedes filtrar conjuntos de datos por dominio, n√∫mero de clases, tipos de anotaci√≥n y m√°s. Utiliza estos filtros para reducir los conjuntos de datos que se ajusten a tus criterios.

**Paso 3. Evaluar Conjuntos de Datos Individuales**

Una vez que tengas una lista corta, eval√∫a cada conjunto de datos individualmente. Busca:

**Calidad de Anotaci√≥n**: Verifica si las anotaciones son precisas y consistentes.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/3.png" style={{width:1000, height:'auto'}}/></div>

**Tama√±o del Conjunto de Datos**: Aseg√∫rate de que el conjunto de datos sea lo suficientemente grande para que tu modelo aprenda efectivamente pero no demasiado grande para manejar.

**Balance de Clases**: El conjunto de datos deber√≠a idealmente tener un n√∫mero equilibrado de ejemplos para cada clase.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/4.png" style={{width:1000, height:'auto'}}/></div>

**Licencia**: Revisa la licencia del conjunto de datos para asegurarte de que puedas usarlo seg√∫n lo previsto.

**Documentaci√≥n**: Revisa cualquier documentaci√≥n o metadatos que vengan con el conjunto de datos para entender mejor su contenido y cualquier paso de preprocesamiento que ya se haya aplicado.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/5.png" style={{width:1000, height:'auto'}}/></div>

:::tip
Puedes conocer la condici√≥n del modelo a trav√©s de **[Roboflow Health Check](https://docs.roboflow.com/datasets/dataset-health-check)**.
:::

**Paso 4. Descargar la Muestra**

Si encuentras el conjunto de datos de tu elecci√≥n, entonces tienes la opci√≥n de descargarlo y usarlo. Roboflow usualmente te permite descargar una muestra del conjunto de datos. Prueba la muestra para ver si se integra bien con tu flujo de trabajo y si es adecuada para tu modelo.

Para continuar con los pasos subsiguientes, recomendamos que exportes el conjunto de datos en el formato mostrado a continuaci√≥n.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/6.png" style={{width:1000, height:'auto'}}/></div>

Entonces obtendr√°s la **URL Raw** para este modelo, mantenla segura, usaremos ese enlace en el paso de entrenamiento del modelo un poco m√°s tarde.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/26.png" style={{width:1000, height:'auto'}}/></div>


:::caution
Si est√°s usando Roboflow por primera vez y no tienes absolutamente ning√∫n juicio sobre la selecci√≥n de conjuntos de datos, el paso de entrenar un modelo con un conjunto de datos para realizar una prueba inicial para ver el rendimiento puede ser esencial. Esto puede ayudarte a evaluar si el conjunto de datos cumplir√° con tus requisitos.

Si el conjunto de datos cumple con tus requisitos y funciona bien en las pruebas iniciales, entonces es probable que sea adecuado para tu proyecto. De lo contrario, puede que necesites continuar tu b√∫squeda o considerar expandir el conjunto de datos con m√°s im√°genes.
:::

</TabItem>

<TabItem value="Use your own images as a dataset" label="Usar tus propias im√°genes como conjunto de datos">

Aqu√≠, usar√© la imagen de gesto piedra-papel-tijeras como demostraci√≥n para guiarte a trav√©s de las tareas de carga de im√°genes, etiquetado y exportaci√≥n de un conjunto de datos en Roboflow.

:::note
Recomendamos encarecidamente que uses XIAO ESP32S3 para tomar fotos de tu conjunto de datos, lo cual es mejor para XIAO ESP32S3. Un programa de muestra para que XIAO ESP32S3 Sense tome fotos se puede encontrar en el enlace Wiki a continuaci√≥n.

<div class="get_one_now_container" style={{textAlign: 'center'}}>
    <a class="get_one_now_item" href="https://wiki.seeedstudio.com/es/xiao_esp32s3_camera_usage/#taking-photos-with-the-camera" target="_blank" rel="noopener noreferrer">
            <strong><span><font color={'FFFFFF'} size={"4"}>Ir al Wiki</font></span></strong>
    </a>
</div>
:::

**Paso 2. Crear un Nuevo Proyecto y Subir im√°genes**

Una vez que hayas iniciado sesi√≥n en Roboflow, haz clic en **Create Project**.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/7.png" style={{width:1000, height:'auto'}}/></div>

Nombra tu proyecto (por ejemplo, "Piedra-Papel-Tijeras"). Define tu proyecto como **Object Detection**. Establece las **Output Labels** como **Categorical** (ya que Piedra, Papel y Tijeras son categor√≠as distintas).

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/8.png" style={{width:1000, height:'auto'}}/></div>

Ahora es momento de subir tus im√°genes de gestos de mano.

Recopila im√°genes de los gestos de piedra, papel y tijeras. Aseg√∫rate de tener una variedad de fondos y condiciones de iluminaci√≥n. En la p√°gina de tu proyecto, haz clic en "Add Images".

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/9.png" style={{width:1000, height:'auto'}}/></div>

Puedes arrastrar y soltar tus im√°genes o seleccionarlas desde tu computadora. Sube al menos 100 im√°genes de cada gesto para un conjunto de datos robusto.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/10.png" style={{width:1000, height:'auto'}}/></div>

:::tip
**¬øC√≥mo se determina el tama√±o del conjunto de datos?**

Generalmente depende de una variedad de factores: modelo de tarea, complejidad de la tarea, pureza de los datos, y as√≠ sucesivamente. Por ejemplo, el modelo de detecci√≥n del cuerpo humano involucra un gran n√∫mero de personas, un amplio rango, la tarea es m√°s compleja, por lo que se necesita recopilar m√°s datos.
Otro ejemplo es el modelo de detecci√≥n de gestos, que solo necesita detectar los tres tipos de "piedra", "tijeras" y "papel", y requiere menos categor√≠as, por lo que el conjunto de datos recopilado es de aproximadamente 500.
:::

**Paso 3: Anotar Im√°genes**

Despu√©s de subir, necesitar√°s anotar las im√°genes etiquetando los gestos de mano.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/11.png" style={{width:1000, height:'auto'}}/></div>

Roboflow ofrece tres formas diferentes de etiquetar im√°genes: Auto Label, Roboflow Labeling y Manual Labeling.

- [**Auto Label**](https://blog.roboflow.com/yolo-world-prompting-tips/): Usa un modelo generalizado grande para etiquetar autom√°ticamente las im√°genes.
- **Roboflow Labeling**: Trabaja con un equipo profesional de etiquetadores humanos. Sin vol√∫menes m√≠nimos. Sin compromisos por adelantado. Las anotaciones de Bounding Box comienzan en \$0.04 y las anotaciones de Pol√≠gono comienzan en \$0.08.
- **Manual Labeling**: T√∫ y tu equipo etiquetan sus propias im√°genes.

Lo siguiente describe el m√©todo m√°s com√∫nmente usado de etiquetado manual.

Haz clic en el bot√≥n "Manual Labeling". Roboflow cargar√° la interfaz de anotaci√≥n.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/12.png" style={{width:1000, height:'auto'}}/></div>

Selecciona el bot√≥n "Start Annotating". Dibuja cajas delimitadoras alrededor del gesto de mano en cada imagen.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/13.gif" style={{width:1000, height:'auto'}}/></div>

Etiqueta cada caja delimitadora como "Rock", "Paper", o "Scissors".

Usa el bot√≥n ">" para moverte a trav√©s de tu conjunto de datos, repitiendo el proceso de anotaci√≥n para cada imagen.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/14.gif" style={{width:1000, height:'auto'}}/></div>


**Paso 4: Revisar y Editar Anotaciones**

Es esencial asegurar que las anotaciones sean precisas.

Revisa cada imagen para asegurarte de que las cajas delimitadoras est√©n correctamente dibujadas y etiquetadas. Si encuentras alg√∫n error, selecciona la anotaci√≥n para ajustar la caja delimitadora o cambiar la etiqueta.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/15.png" style={{width:1000, height:'auto'}}/></div>

:::tip
El etiquetado incorrecto afecta el rendimiento general del entrenamiento y puede descartarse si algunos conjuntos de datos no cumplen con los requisitos de etiquetado. Aqu√≠ hay algunas demostraciones de etiquetado deficiente.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/16.png" style={{width:700, height:'auto'}}/></div>
:::

**Paso 5: Generar y Exportar el Conjunto de Datos**

Una vez que todas las im√°genes est√©n anotadas. En Annotate haz clic en el bot√≥n **Add x images to Dataset** en la esquina superior derecha.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/17.png" style={{width:1000, height:'auto'}}/></div>

Luego haz clic en el bot√≥n **Add Images** en la parte inferior de la nueva ventana emergente.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/18.png" style={{width:400, height:'auto'}}/></div>

Haz clic en **Generate** en la barra de herramientas izquierda y haz clic en **Continue** en el tercer paso **Preprocessing**.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/19.png" style={{width:1000, height:'auto'}}/></div>

En la **Augmentation** en el paso 4, selecciona **Mosaic**, que aumenta la generalizaci√≥n.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/20.png" style={{width:1000, height:'auto'}}/></div>

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/21.png" style={{width:1000, height:'auto'}}/></div>

En el paso final **Create**, por favor calcula el n√∫mero de im√°genes razonablemente seg√∫n el boost de Roboflow; en general, mientras m√°s im√°genes tengas, m√°s tiempo toma entrenar el modelo. Sin embargo, tener m√°s im√°genes no necesariamente har√° que el modelo sea m√°s preciso, principalmente depende de si el conjunto de datos es lo suficientemente bueno o no.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/22.png" style={{width:1000, height:'auto'}}/></div>

Haz clic en **Create** para crear una versi√≥n de tu conjunto de datos. Roboflow procesar√° las im√°genes y anotaciones, creando un conjunto de datos versionado. Despu√©s de que se genere el conjunto de datos, haz clic en **Export Dataset**. Elige el formato **COCO** que coincida con los requisitos del modelo que vas a entrenar.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/23.png" style={{width:1000, height:'auto'}}/></div>

Haz clic en **Continue** y luego obtendr√°s la URL Raw para este modelo. Gu√°rdala, usaremos el enlace en el paso de entrenamiento del modelo un poco m√°s tarde.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/27.png" style={{width:1000, height:'auto'}}/></div>


¬°Felicidades! Has usado exitosamente Roboflow para subir, anotar y exportar un conjunto de datos para un modelo de detecci√≥n de gestos de mano de Piedra-Papel-Tijeras. Con tu conjunto de datos listo, puedes proceder a entrenar un modelo de aprendizaje autom√°tico usando plataformas como Google Colab.

Recuerda mantener tu conjunto de datos diverso y bien anotado para mejorar la precisi√≥n de tu modelo futuro. ¬°Buena suerte con el entrenamiento de tu modelo, y divi√©rtete clasificando gestos de mano con el poder de la IA!
</TabItem>
</Tabs>


## Entrenamiento del Modelo Exportado del Conjunto de Datos


### Paso 1. Acceder al Notebook de Colab

Puedes encontrar diferentes tipos de archivos de c√≥digo de modelos de Google Colab en el [Wiki del Asistente de Modelos de SenseCraft](https://wiki.seeedstudio.com/es/ModelAssistant_Introduce_Quick_Start/#model-training). Si no sabes qu√© c√≥digo deber√≠as elegir, puedes elegir cualquiera de ellos, dependiendo de la clase de tu modelo (detecci√≥n de objetos o clasificaci√≥n de im√°genes).

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/24.png" style={{width:1000, height:'auto'}}/></div>

Si a√∫n no has iniciado sesi√≥n en tu cuenta de Google, por favor inicia sesi√≥n para acceder a todas las funcionalidades de Google Colab.

Haz clic en "Connect" para asignar recursos para tu sesi√≥n de Colab.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/25.png" style={{width:1000, height:'auto'}}/></div>

### Paso 2. Agregar tu Conjunto de Datos de Roboflow

Antes de ejecutar oficialmente el bloque de c√≥digo paso a paso, necesitamos modificar el contenido del c√≥digo para que el c√≥digo pueda usar el conjunto de datos que preparamos. Tenemos que proporcionar una URL para descargar el conjunto de datos directamente al sistema de archivos de Colab.

Por favor encuentra la secci√≥n **Download the dataset** en el c√≥digo. Ver√°s el siguiente programa de ejemplo.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/28.png" style={{width:1000, height:'auto'}}/></div>

```sh
%mkdir -p Gesture_Detection_Swift-YOLO_192/dataset 
!wget -c https://universe.roboflow.com/ds/xaMM3ZTeWy?key=5bznPZyI0t -O Gesture_Detection_Swift-YOLO_192/dataset.zip 
!unzip -q Gesture_Detection_Swift-YOLO_192/dataset.zip -d Gesture_Detection_Swift-YOLO_192/dataset
```

Este fragmento de c√≥digo se utiliza para crear un directorio, descargar un conjunto de datos desde Roboflow y descomprimirlo en el directorio reci√©n creado dentro de un entorno de Google Colab. Aqu√≠ tienes un desglose de lo que hace cada l√≠nea:

1. `%mkdir -p Gesture_Detection_Swift-YOLO_192/dataset`:
   - Esta l√≠nea crea un nuevo directorio llamado `Gesture_Detection_Swift-YOLO_192` y un subdirectorio llamado `dataset`. La bandera `-p` asegura que el comando no devuelva un error si el directorio ya existe y crea cualquier directorio padre necesario.

2. `!wget -c https://universe.roboflow.com/ds/xaMM3ZTeWy?key=5bznPZyI0t -O Gesture_Detection_Swift-YOLO_192/dataset.zip`:
   - Esta l√≠nea utiliza `wget`, una utilidad de l√≠nea de comandos, para descargar el conjunto de datos desde la URL de Roboflow proporcionada. La bandera `-c` permite que la descarga se reanude si se interrumpe. La bandera `-O` especifica la ubicaci√≥n de salida y el nombre de archivo para el archivo descargado, en este caso, `Gesture_Detection_Swift-YOLO_192/dataset.zip`.

3. `!unzip -q Gesture_Detection_Swift-YOLO_192/dataset.zip -d Gesture_Detection_Swift-YOLO_192/dataset`:
   - Esta l√≠nea utiliza el comando `unzip` para extraer el contenido del archivo `dataset.zip` en el directorio `dataset` que se cre√≥ anteriormente. La bandera `-q` ejecuta el comando `unzip` en modo silencioso, suprimiendo la mayor√≠a de los mensajes de salida.

Para personalizar este c√≥digo para tu propio enlace de modelo desde Roboflow:

1. Reemplaza `Gesture_Detection_Swift-YOLO_192` con el nombre de directorio deseado donde quieras almacenar tu conjunto de datos.

2. Reemplaza la URL del conjunto de datos de Roboflow (`https://universe.roboflow.com/ds/xaMM3ZTeWy?key=5bznPZyI0t`) con el enlace a tu conjunto de datos exportado (Es la URL Raw que obtuvimos en el [√∫ltimo paso en Conjuntos de Datos Etiquetados](#choose-how-you-get-your-dataset)). Aseg√∫rate de incluir el par√°metro key si es requerido para el acceso.

3. Ajusta el nombre de archivo de salida en el comando `wget` si es necesario (`-O tu_directorio/tu_nombre_archivo.zip`).

4. Aseg√∫rate de que el directorio de salida en el comando `unzip` coincida con el directorio que creaste y el nombre de archivo coincida con el que estableciste en el comando `wget`.

:::caution
Si cambias el nombre de un directorio de carpeta `Gesture_Detection_Swift-YOLO_192`, ten en cuenta que necesitar√°s hacer cambios a otros nombres de directorio en el c√≥digo que se usaron antes del cambio, ¬°de lo contrario puede ocurrir un error!
:::

### Paso 3. Ajuste de par√°metros del modelo

El siguiente paso es ajustar los par√°metros de entrada del modelo. Por favor ve a la secci√≥n Entrenar un modelo con SSCMA y ver√°s el siguiente fragmento de c√≥digo.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/29.png" style={{width:1000, height:'auto'}}/></div>

```sh
!sscma.train configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py \
--cfg-options  \
    work_dir=Gesture_Detection_Swift-YOLO_192 \
    num_classes=3 \
    epochs=10  \
    height=192 \
    width=192 \
    data_root=Gesture_Detection_Swift-YOLO_192/dataset/ \
    load_from=Gesture_Detection_Swift-YOLO_192/pretrain.pth
```


Este comando se utiliza para iniciar el proceso de entrenamiento de un modelo de aprendizaje autom√°tico, espec√≠ficamente un modelo YOLO (You Only Look Once), usando el framework SSCMA (Seeed Studio SenseCraft Model Assistant). El comando incluye varias opciones para configurar el proceso de entrenamiento. Esto es lo que hace cada parte:

- `!sscma.train` es el comando para iniciar el entrenamiento dentro del framework SSCMA.

- `configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py` especifica el archivo de configuraci√≥n para el entrenamiento, que incluye configuraciones como la arquitectura del modelo, el cronograma de entrenamiento, las estrategias de aumento de datos, etc.

- `--cfg-options` te permite sobrescribir las configuraciones predeterminadas especificadas en el archivo `.py` con las que proporciones en la l√≠nea de comandos.

- `work_dir=Gesture_Detection_Swift-YOLO_192` establece el directorio donde se almacenar√°n las salidas del entrenamiento, como los registros y los puntos de control del modelo guardado.

- `num_classes=3` especifica el n√∫mero de clases que el modelo debe ser entrenado para reconocer. Depende del n√∫mero de etiquetas que tengas, por ejemplo piedra, papel, tijeras deber√≠an ser tres etiquetas.

- `epochs=10` establece el n√∫mero de ciclos de entrenamiento (√©pocas) a ejecutar. Los valores recomendados est√°n entre 50 y 100.

- `height=192` y `width=192` establecen la altura y el ancho de las im√°genes de entrada que el modelo espera.

:::caution
Realmente no recomendamos que cambies el tama√±o de imagen en el c√≥digo de Colab, ya que este valor es un tama√±o de conjunto de datos m√°s apropiado que hemos verificado que es una combinaci√≥n de tama√±o, precisi√≥n y velocidad de inferencia. Si est√°s usando un conjunto de datos que no es de este tama√±o, y puedes querer considerar cambiar el tama√±o de imagen para asegurar la precisi√≥n, entonces por favor no excedas 240x240.
:::

- `data_root=Gesture_Detection_Swift-YOLO_192/dataset/` define la ruta al directorio donde se encuentran los datos de entrenamiento.

- `load_from=Gesture_Detection_Swift-YOLO_192/pretrain.pth` proporciona la ruta a un archivo de punto de control de modelo preentrenado desde el cual el entrenamiento debe reanudarse o usar como punto de partida para el aprendizaje por transferencia.

Para personalizar este comando para tu propio entrenamiento, deber√≠as:

1. Reemplazar `configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py` con la ruta a tu propio archivo de configuraci√≥n si tienes uno personalizado.

2. Cambiar `work_dir` al directorio donde quieres que se guarden las salidas de tu entrenamiento.

3. Actualizar `num_classes` para que coincida con el n√∫mero de clases en tu propio conjunto de datos. Depende del n√∫mero de etiquetas que tengas, por ejemplo piedra, papel, tijeras deber√≠an ser tres etiquetas.

4. Ajustar `epochs` al n√∫mero deseado de √©pocas de entrenamiento para tu modelo. Los valores recomendados est√°n entre 50 y 100.

5. Establecer `height` y `width` para que coincidan con las dimensiones de las im√°genes de entrada para tu modelo.

6. Cambiar `data_root` para que apunte al directorio ra√≠z de tu conjunto de datos.

7. Si tienes un archivo de modelo preentrenado diferente, actualiza la ruta `load_from` en consecuencia.

### Paso 4. Ejecutar el c√≥digo de Google Colab

La forma de ejecutar el bloque de c√≥digo es hacer clic en el bot√≥n de reproducci√≥n en la esquina superior izquierda del bloque de c√≥digo.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/30.png" style={{width:1000, height:'auto'}}/></div>

El bloque de c√≥digo se ejecutar√° despu√©s de que hagas clic en el bot√≥n, y si todo va bien, ver√°s la se√±al de que la ejecuci√≥n del bloque de c√≥digo est√° completa - aparece un s√≠mbolo de marca de verificaci√≥n a la izquierda del bloque. Como se muestra en la figura es el efecto despu√©s de que se complete la ejecuci√≥n del primer bloque de c√≥digo.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/31.png" style={{width:1000, height:'auto'}}/></div>

Si encuentras el mismo mensaje de error que el m√≠o en la imagen de arriba, por favor verifica que est√©s usando una **GPU T4**, por favor **no uses CPU** para este proyecto.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/32.png" style={{width:400, height:'auto'}}/></div>

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/33.png" style={{width:600, height:'auto'}}/></div>

Luego, vuelve a ejecutar el bloque de c√≥digo. Para el primer bloque de c√≥digo, si todo va bien, ver√°s el resultado mostrado a continuaci√≥n.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/34.png" style={{width:1000, height:'auto'}}/></div>

A continuaci√≥n, ejecuta todos los bloques de c√≥digo desde **Download the pretrain model weights file** hasta **Export the model**. Y por favor aseg√∫rate de que cada bloque de c√≥digo est√© libre de errores.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/36.png" style={{width:400, height:'auto'}}/></div>

:::note
Las advertencias que aparecen en el c√≥digo pueden ser ignoradas.
:::

### Paso 5. Evaluar el modelo

Cuando llegues a la secci√≥n **Evaluate the model**, tienes la opci√≥n de ejecutar el bloque de c√≥digo **Evaluate the TFLite INT8 model**.

:::tip
Evaluar el modelo TFLite INT8 implica probar las predicciones del modelo cuantizado contra un conjunto de datos de prueba separado para medir su precisi√≥n y m√©tricas de rendimiento, evaluar el impacto de la cuantizaci√≥n en la precisi√≥n del modelo, y perfilar su velocidad de inferencia y uso de recursos para asegurar que cumple con las restricciones de despliegue para dispositivos de borde.
:::

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/35.png" style={{width:1000, height:'auto'}}/></div>

El siguiente fragmento es la parte v√°lida del resultado despu√©s de que ejecut√© este bloque de c√≥digo.

```
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.929
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.474
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.456
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.515
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.529
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537
03/19 01:38:43 - mmengine - INFO - bbox_mAP_copypaste: 0.450 0.929 0.361 -1.000 0.474 0.456
{'coco/bbox_mAP': 0.45, 'coco/bbox_mAP_50': 0.929, 'coco/bbox_mAP_75': 0.361, 'coco/bbox_mAP_s': -1.0, 'coco/bbox_mAP_m': 0.474, 'coco/bbox_mAP_l': 0.456}
FPS: 128.350449 fram/s
```

Los resultados de evaluaci√≥n incluyen una serie de m√©tricas de Precisi√≥n Promedio (AP) y Recuperaci√≥n Promedio (AR), calculadas para diferentes umbrales de Intersecci√≥n sobre Uni√≥n (IoU) y tama√±os de objetos, que se utilizan com√∫nmente para evaluar el rendimiento de modelos de detecci√≥n de objetos.

1. **AP@[IoU=0.50:0.95 | area=all | maxDets=100] = 0.450**
   - Esta puntuaci√≥n es la precisi√≥n promedio del modelo a trav√©s de un rango de umbrales IoU desde 0.50 hasta 0.95, incrementado en 0.05. Un AP de 0.450 indica que tu modelo tiene una precisi√≥n moderada a trav√©s de este rango. Esta es una m√©trica clave com√∫nmente utilizada para el conjunto de datos COCO.

2. **AP@[IoU=0.50 | area=all | maxDets=100] = 0.929**
   - Con un umbral IoU de 0.50, el modelo logra una alta precisi√≥n promedio de 0.929, sugiriendo que detecta objetos muy precisamente bajo un criterio de coincidencia m√°s permisivo.

3. **AP@[IoU=0.75 | area=all | maxDets=100] = 0.361**
   - Con un umbral IoU m√°s estricto de 0.75, la precisi√≥n promedio del modelo baja a 0.361, indicando una disminuci√≥n en el rendimiento bajo criterios de coincidencia m√°s estrictos.

4. **AP@[IoU=0.50:0.95 | area=small/medium/large | maxDets=100]**
   - Las puntuaciones AP var√≠an para objetos de diferentes tama√±os. Sin embargo, el AP para objetos peque√±os es -1.000, lo que podr√≠a indicar una falta de datos de evaluaci√≥n para objetos peque√±os o un rendimiento pobre del modelo en la detecci√≥n de objetos peque√±os. Las puntuaciones AP para objetos medianos y grandes son 0.474 y 0.456, respectivamente, sugiriendo que el modelo detecta objetos medianos y grandes relativamente mejor.

5. **AR@[IoU=0.50:0.95 | area=all | maxDets=1/10/100]**
   - Las tasas de recuperaci√≥n promedio para diferentes valores de `maxDets` son bastante consistentes, variando desde 0.515 hasta 0.529, indicando que el modelo recupera confiablemente la mayor√≠a de las instancias verdaderas positivas.

6. **FPS: 128.350449 fram/s**
   - El modelo procesa im√°genes a una velocidad muy r√°pida de aproximadamente 128.35 cuadros por segundo durante la inferencia, indicando potencial para aplicaciones en tiempo real o casi en tiempo real.

En general, el modelo se desempe√±a excelentemente con un IoU de 0.50 y moderadamente con un IoU de 0.75. Se desempe√±a mejor en la detecci√≥n de objetos medianos y grandes pero puede tener problemas detectando objetos peque√±os. Adicionalmente, el modelo infiere a alta velocidad, haci√©ndolo adecuado para escenarios que requieren procesamiento r√°pido. Si detectar objetos peque√±os es cr√≠tico en una aplicaci√≥n, podr√≠amos necesitar optimizar m√°s el modelo o recopilar m√°s datos de objetos peque√±os para mejorar el rendimiento.

### Paso 6. Descargar el archivo del modelo exportado

Despu√©s de la secci√≥n **Export the model**, obtendr√°s los archivos del modelo en varios formatos, que se almacenar√°n en la carpeta ModelAssistant por defecto. En este tutorial, el directorio almacenado es **Gesture_Detection_Swift_YOLO_192**.

:::tip
A veces Google Colab no actualiza autom√°ticamente el contenido de una carpeta. En este caso puede que necesites actualizar el directorio de archivos haciendo clic en el √≠cono de actualizaci√≥n en la esquina superior izquierda.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/39.png" style={{width:500, height:'auto'}}/></div>

:::

En el directorio anterior, los archivos del modelo **.tflite** est√°n disponibles para XIAO ESP32S3 y Grove Vision AI V2. Para XIAO ESP32S3 Sense, aseg√∫rate de seleccionar el archivo del modelo que usa el formato **xxx_int8.tflite**. Ning√∫n otro formato puede ser usado por XIAO ESP32S3 Sense.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/37.png" style={{width:400, height:'auto'}}/></div>

Una vez que hayas encontrado los archivos del modelo, por favor desc√°rgalos localmente a tu computadora tan pronto como sea posible, ¬°Google Colab puede vaciar tu directorio de almacenamiento si permaneces inactivo por mucho tiempo!

As√≠ que con los pasos realizados aqu√≠, hemos exportado exitosamente archivos de modelo que pueden ser soportados por XIAO ESP32S3, a continuaci√≥n vamos a desplegar el modelo al dispositivo.

## Subir modelos a trav√©s de SenseCraft Model Assistant

### Paso 7. Subir modelo personalizado a XIAO ESP32S3

A continuaci√≥n, llegamos a la p√°gina de Model Assistant.

<div class="get_one_now_container" style={{textAlign: 'center'}}>
    <a class="get_one_now_item" href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process" target="_blank" rel="noopener noreferrer">
            <strong><span><font color={'FFFFFF'} size={"4"}>Model Assistant üñ±Ô∏è</font></span></strong>
    </a>
</div>
<br></br>

Por favor conecta el dispositivo despu√©s de seleccionar XIAO ESP32S3 y luego selecciona **Upload Custom AI Model** en la parte inferior de la p√°gina.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/xiaos3-sscma/1.png" style={{width:1000, height:'auto'}}/></div>

Entonces necesitar√°s preparar el nombre del modelo, el archivo del modelo y las etiquetas. Quiero destacar aqu√≠ c√≥mo se determina este elemento del ID de etiqueta.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/41.png" style={{width:500, height:'auto'}}/></div>

**Si est√°s descargando el conjunto de datos de Roboflow directamente**

Si descargaste el conjunto de datos de Roboflow directamente, entonces puedes ver las diferentes categor√≠as y su orden en la p√°gina de Health Check. Solo sigue el orden ingresado aqu√≠.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/42.png" style={{width:1000, height:'auto'}}/></div>

:::tip
No necesitas llenar los n√∫meros en **ID:Object**, solo llena el nombre de la categor√≠a directamente, los n√∫meros y dos puntos delante de las categor√≠as en la imagen se a√±aden autom√°ticamente.
:::

**Si est√°s usando un conjunto de datos personalizado**

Si est√°s usando un conjunto de datos personalizado, entonces puedes ver las diferentes categor√≠as y su orden en la p√°gina de Health Check. Solo sigue el orden ingresado aqu√≠.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/43.png" style={{width:1000, height:'auto'}}/></div>

:::tip
No necesitas llenar los n√∫meros en **ID:Object**, solo llena el nombre de la categor√≠a directamente, los n√∫meros y dos puntos delante de las categor√≠as en la imagen se a√±aden autom√°ticamente.
:::

Luego haz clic en Send Model en la esquina inferior derecha. Esto puede tomar alrededor de 3 a 5 minutos aproximadamente. Si todo va bien, entonces puedes ver los resultados de tu modelo en las ventanas de Model Name y Preview arriba.

<div style={{textAlign:'center'}}><img src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/44.png" style={{width:1000, height:'auto'}}/></div>

Habiendo llegado hasta aqu√≠, felicitaciones, has sido capaz de entrenar y desplegar exitosamente un modelo propio.

## Protocolos comunes y aplicaciones del modelo

Durante el proceso de subir un modelo personalizado, adem√°s de los archivos del modelo que podemos visualizar siendo subidos, tambi√©n est√° el firmware del dispositivo que necesita ser transferido al dispositivo. En el firmware del dispositivo, hay un conjunto de protocolos de comunicaci√≥n establecidos que especifican el formato de salida de los resultados del modelo, y lo que el usuario puede hacer con los modelos.

Debido a problemas de espacio, no expandiremos los detalles espec√≠ficos de estos protocolos en este wiki, detallaremos esta secci√≥n a trav√©s de documentaci√≥n en Github. Si est√°s interesado en un desarrollo m√°s profundo, por favor ve aqu√≠.

<div class="get_one_now_container" style={{textAlign: 'center'}}>
    <a class="get_one_now_item" href="https://github.com/Seeed-Studio/SSCMA-Micro/blob/dev/docs/protocol/at_protocol.md" target="_blank" rel="noopener noreferrer">
            <strong><span><font color={'FFFFFF'} size={"4"}>SenseCraft Protocols</font></span></strong>
    </a>
</div>
<br></br>

## Soluci√≥n de problemas

### 1. ¬øQu√© pasa si sigo los pasos y obtengo resultados del modelo menos que satisfactorios?

Si la precisi√≥n de reconocimiento de tu modelo no es satisfactoria, podr√≠as diagnosticar y mejorarlo considerando los siguientes aspectos:

1. **Calidad y Cantidad de Datos**
   - **Problema**: El conjunto de datos podr√≠a ser demasiado peque√±o o carecer de diversidad, o podr√≠a haber inexactitudes en las anotaciones.
   - **Soluci√≥n**: Aumenta el tama√±o y la diversidad de los datos de entrenamiento, y realiza limpieza de datos para corregir cualquier error de anotaci√≥n.

2. **Proceso de Entrenamiento**
   - **Problema**: El tiempo de entrenamiento podr√≠a ser insuficiente, o la tasa de aprendizaje podr√≠a estar configurada incorrectamente, impidiendo que el modelo aprenda efectivamente.
   - **Soluci√≥n**: Aumenta el n√∫mero de √©pocas de entrenamiento, ajusta la tasa de aprendizaje y otros hiperpar√°metros, e implementa parada temprana para evitar el sobreajuste.

3. **Desequilibrio de Clases**
   - **Problema**: Algunas clases tienen significativamente m√°s muestras que otras, llevando a un sesgo del modelo hacia la clase mayoritaria.
   - **Soluci√≥n**: Usa pesos de clase, sobremuestrea las clases minoritarias, o submuestrea las clases mayoritarias para equilibrar los datos.

Al analizar exhaustivamente e implementar mejoras dirigidas, puedes mejorar progresivamente la precisi√≥n de tu modelo. Recuerda usar un conjunto de validaci√≥n para probar el rendimiento del modelo despu√©s de cada modificaci√≥n para asegurar la efectividad de tus mejoras.

### 2. ¬øPor qu√© veo el mensaje **Invoke failed** en el despliegue de SenseCraft despu√©s de seguir los pasos en la Wiki?

Si encuentras un Invoke failed, entonces has entrenado un modelo que no cumple con los requisitos para usar con el dispositivo. Por favor enf√≥cate en las siguientes √°reas.

1. Por favor verifica si has modificado el tama√±o de imagen de Colab. El tama√±o de compresi√≥n predeterminado es **192x192**, Grove Vision AI V2 requiere que el tama√±o de imagen sea comprimido como cuadrado, por favor no uses tama√±o no cuadrado para compresi√≥n. Tampoco uses tama√±o demasiado grande *(no se recomienda m√°s de 240x240)*.

<!-- 2. El archivo del modelo para Grove Vision AI V2 debe tener el sufijo **int8_vela.tflite**. Por favor no uses archivos de modelo en otros formatos. Esto incluye **int8.tflite, que tampoco est√° disponible** para Grove Vision AI V2. -->

## Soporte T√©cnico y Discusi√≥n de Productos

¬°Gracias por elegir nuestros productos! Estamos aqu√≠ para proporcionarte diferentes tipos de soporte para asegurar que tu experiencia con nuestros productos sea lo m√°s fluida posible. Ofrecemos varios canales de comunicaci√≥n para atender diferentes preferencias y necesidades.

<div class="button_tech_support_container">
<a href="https://forum.seeedstudio.com/" class="button_forum"></a>
<a href="https://www.seeedstudio.com/contacts" class="button_email"></a>
</div>

<div class="button_tech_support_container">
<a href="https://discord.gg/eWkprNDMU7" class="button_discord"></a>
<a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" class="button_discussion"></a>
</div>

