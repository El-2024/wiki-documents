---
description: Vision AI with Customizable Models
title: Vision AI con Modelos Personalizados
keywords:
- SenseCap
image: https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png
slug: /es/Vision_AI_with_Customizable_Models
last_update:
  date: 06/12/2025
  author: Guillermo
---

# Entrena y Despliega Tu Propio Modelo de IA en Grove - Vision AI

## Actualizable a Sensores Industriales

Con el [controlador SenseCAP S2110](https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html) y el [registrador de datos S2100](https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html), puedes convertir fácilmente el Grove en un sensor LoRaWAN®. Seeed no solo te ayuda con la fase de prototipado, sino que también te ofrece la posibilidad de expandir tu proyecto con la serie SenseCAP de [sensores industriales](https://www.seeedstudio.com/catalogsearch/result/?q=sensecap&categories=SenseCAP&application=Temperature%2FHumidity~Soil~Gas~Light~Weather~Water~Automation~Positioning~Machine%20Learning~Voice%20Recognition&compatibility=SenseCAP) robustos.

La carcasa IP66, la configuración por Bluetooth, la compatibilidad con la red global LoRaWAN®, la batería incorporada de 19 Ah y el sólido soporte desde la app hacen del [SenseCAP S210x](https://www.seeedstudio.com/catalogsearch/result/?q=S21&categories=SenseCAP~LoRaWAN%20Device&product_module=Device) la mejor opción para aplicaciones industriales.  
La serie incluye sensores para humedad del suelo, temperatura y humedad del aire, intensidad lumínica, CO₂, EC y una estación meteorológica 8 en 1. Prueba el más reciente SenseCAP S210x para tu próximo proyecto industrial exitoso.

<table style={{marginLeft: 'auto', marginRight: 'auto'}}>
  <tbody><tr><td colSpan={4} bgcolor="#0e3c49" align="center"><font color="white" size={4}><strong>SenseCAP Industrial Sensor</strong></font></td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2100.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&S2103.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2102.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&S2103.png" /></a></div>
      </td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank"><strong>S2100 <br /> Data Logger</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank"><strong>S2101 <br /> Temp Aire &amp; Humedad</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank"><strong>S2102 <br /> Luz</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank"><strong>S2103 <br /> Temp Aire &amp; Humedad &amp; CO2</strong></a></td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2104.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2105.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2110.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2120.png" /></a></div>
      </td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank"><strong>S2104 <br /> Humedad de suelo &amp; Temp</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank"><strong>S2105 <br /> Humedad de suelo &amp; Temp &amp; EC</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank"><strong>S2110 <br /> LoRaWAN® Controller</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank"><strong>S2120 <br /> Estación Meteorológica 8-en-1</strong></a></td>
    </tr>
  </tbody></table>

## Visión General

En esta wiki, te enseñaremos cómo entrenar tu propio modelo de inteligencia artificial para una aplicación específica y luego desplegarlo fácilmente en el módulo Grove - Vision AI. ¡Vamos a comenzar!

## Introducción al Hardware

Usaremos principalmente el módulo Grove - Vision AI a lo largo de esta wiki. Así que primero, familiaricémonos con el hardware.

### Módulo Grove - Vision AI

El [Módulo Grove - Vision AI](https://www.seeedstudio.com/Grove-Vision-AI-Module-p-5457.html) es una cámara de inteligencia artificial del tamaño de un pulgar, con un sensor personalizado que ya tiene instalado un algoritmo de aprendizaje automático para detección de personas y otros modelos personalizados.  
Puede desplegarse y mostrarse fácilmente en cuestión de minutos, funciona en un modelo de consumo ultrabajo, y ofrece dos métodos de transmisión de señal junto con múltiples módulos integrados. Todo esto lo convierte en una excelente opción para iniciarse en el uso de cámaras con inteligencia artificial.


<!-- <div align=center><img width=350 src="https://files.seeedstudio.com/wiki/Wio-Terminal-Developer-for-helium/camera.jpg"/></div> -->

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/Wio-Terminal-Developer-for-helium/camera.jpg" alt="pir" width={600} height="auto" /></p>

## Introducción al Software

En esta wiki utilizaremos las siguientes tecnologías de software:

- Roboflow – para la anotación
- YOLOv5 – para el entrenamiento
- TensorFlow Lite – para la inferencia

<!-- <div align=center><img width=600 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/57.png"/></div> -->

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/57.png" alt="pir" width={600} height="auto" /></p>

### ¿Qué es Roboflow?

[Roboflow](https://roboflow.com) es una herramienta de anotación en línea. Esta herramienta permite anotar fácilmente todas tus imágenes, aplicar procesamiento adicional a estas imágenes y exportar el conjunto de datos etiquetado a diferentes formatos como YOLOv5 PyTorch, Pascal VOC y más. Roboflow también ofrece conjuntos de datos públicos listos para usar.

### ¿Qué es YOLOv5?

YOLO es la abreviatura de "You Only Look Once". Es un algoritmo que detecta y reconoce varios objetos en una imagen en tiempo real. [YOLOv5](https://ultralytics.com/yolov5) de Ultralytics es una versión de YOLO basada en el framework PyTorch.

### ¿Qué es TensorFlow Lite?

[TensorFlow Lite](https://www.tensorflow.org/lite) es un framework de aprendizaje profundo de código abierto, listo para producción y multiplataforma. Convierte un modelo preentrenado en TensorFlow a un formato especial optimizado para velocidad o almacenamiento. Este modelo puede desplegarse en dispositivos de borde como móviles Android o iOS, o dispositivos embebidos basados en Linux como Raspberry Pi o microcontroladores, permitiendo realizar inferencias directamente en el borde.

## Estructura de la Wiki

Esta wiki está dividida en tres secciones principales:

1. [Entrena tu propio modelo de IA con un conjunto de datos público](#jump1)
2. [Entrena tu propio modelo de IA con tu propio conjunto de datos](#jump2)
3. [Despliega el modelo entrenado en el módulo Grove - Vision AI](#jump3)

La primera sección es la forma más rápida de construir tu modelo de IA con la menor cantidad de pasos. La segunda sección requiere más tiempo y esfuerzo, pero sin duda vale la pena por el conocimiento adquirido. La tercera sección sobre el despliegue del modelo puede realizarse después de completar cualquiera de las dos primeras.

Entonces, hay dos formas de seguir esta wiki:

1. Seguir la [sección 1](#jump1) y luego la [sección 3](#jump3) – camino rápido
2. Seguir la [sección 2](#jump2) y luego la [sección 3](#jump3) – camino lento

Sin embargo, se recomienda comenzar con la primera opción y luego pasar a la segunda.

## <span id="jump1">1. Entrena tu propio modelo de IA con un conjunto de datos público</span>

El primer paso en un proyecto de detección de objetos es obtener datos para el entrenamiento. Puedes descargar conjuntos de datos públicos disponibles o crear los tuyos propios.

Pero, ¿cuál es la forma más rápida y sencilla de comenzar con la detección de objetos?  
Usar conjuntos de datos públicos puede ahorrarte mucho tiempo que de otro modo gastarías recolectando y anotando datos tú mismo.  
Estos conjuntos ya están anotados y listos para usarse, permitiéndote enfocarte directamente en las aplicaciones de visión con IA.

### Preparación del hardware

- Módulo Grove - Vision AI
- Cable USB tipo C
- Computadora con Windows, Linux o Mac con acceso a internet

### Preparación del software

- No es necesario instalar software adicional

### Usa un conjunto de datos anotado públicamente

Puedes descargar una gran variedad de conjuntos de datos públicos como el [conjunto de datos COCO](https://cocodataset.org), el [conjunto de datos Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC), entre muchos otros. [Roboflow Universe](https://universe.roboflow.com) es una plataforma recomendada que ofrece una amplia gama de conjuntos de datos. Cuenta con [más de 90,000 conjuntos de datos y más de 66 millones de imágenes](https://blog.roboflow.com/computer-vision-datasets-and-apis) disponibles para desarrollar modelos de visión por computadora. También puedes buscar simplemente **conjuntos de datos de código abierto** en Google y elegir entre una variedad de opciones disponibles.

- **Paso 1.** Visita [esta URL](https://universe.roboflow.com/lakshantha-dissanayake/apple-detection-5z37o/dataset/1) para acceder a un conjunto de datos de detección de manzanas disponible públicamente en Roboflow Universe.

- **Paso 2.** Haz clic en **Create Account** para crear una cuenta de Roboflow.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/53.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/53.png" alt="pir" width={600} height="auto" /></p>

- **Paso 3.** Haz clic en **Download**, selecciona **YOLO v5 PyTorch** como **Formato**, luego haz clic en **show download code** y finalmente en **Continue**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/51.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/51.png" alt="pir" width={600} height="auto" /></p>
Esto generará un fragmento de código que utilizaremos más adelante en el entrenamiento con Google Colab. Por favor, mantén esta ventana abierta en segundo plano.

<!-- <div align=center><img width=700 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/52.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/52.png" alt="pir" width={600} height="auto" /></p>

### Entrena usando YOLOv5 en Google Colab

Después de haber elegido un conjunto de datos público, es momento de entrenarlo. Aquí usaremos el entorno de Google Colaboratory para realizar el entrenamiento en la nube. Además, utilizamos la API de Roboflow dentro de Colab para descargar fácilmente nuestro conjunto de datos.

Haz clic [aquí](https://colab.research.google.com/gist/lakshanthad/b47a1d1a9b4fac43449948524de7d374/yolov5-training-for-sensecap-a1101.ipynb) para abrir un espacio de trabajo de Google Colab ya preparado. Sigue los pasos indicados en el notebook y ejecuta las celdas de código una por una.

**Nota:** En Google Colab, en la celda de código del **Paso 4**, puedes copiar directamente el fragmento de código generado por Roboflow, como se mencionó anteriormente.

Este notebook te guiará por los siguientes pasos:

- Configurar el entorno de entrenamiento
- Descargar un conjunto de datos
- Realizar el entrenamiento
- Descargar el modelo entrenado

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/18.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/18.png" alt="pir" width={600} height="auto" /></p>
Para un conjunto de datos de detección de manzanas con 699 imágenes, el proceso de entrenamiento tomó aproximadamente 7 minutos en Google Colab utilizando una GPU NVIDIA Tesla T4 con 16 GB de memoria.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/43.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/43.png" alt="pir" width={600} height="auto" /></p>
Si seguiste el proyecto de Colab mencionado anteriormente, sabrás que puedes cargar hasta 4 modelos en el dispositivo al mismo tiempo. Sin embargo, ten en cuenta que solo se puede cargar **un modelo a la vez**. Esto puede especificarse según el usuario y se explicará más adelante en este wiki.

### Desplegar e inferir

Si deseas saltar directamente a la **sección 3**, donde se explica cómo desplegar el modelo de IA entrenado en el módulo Grove - Vision AI y realizar inferencias, [haz clic aquí](#jump3).

### Anotar un conjunto de datos usando Roboflow

Si decides usar tu propio conjunto de datos, necesitarás anotar todas las imágenes. Anotar significa simplemente dibujar cajas rectangulares alrededor de cada objeto que deseamos detectar y asignarles una etiqueta. A continuación, explicamos cómo hacerlo utilizando Roboflow.

[Roboflow](https://roboflow.com) es una herramienta en línea para anotación de imágenes. Podemos importar directamente material de video que hayamos grabado a Roboflow, y este lo convertirá en una serie de imágenes. Esta herramienta es muy conveniente ya que nos permite dividir fácilmente el conjunto de datos en "entrenamiento, validación y prueba". Además, permite aplicar procesamiento adicional a las imágenes después de etiquetarlas.

También puede exportar fácilmente el conjunto de datos etiquetado en el **formato YOLOv5 PyTorch**, ¡que es exactamente lo que necesitamos!

En este wiki, utilizaremos un conjunto de datos con imágenes que contienen manzanas, de forma que podamos detectarlas más adelante y también realizar un conteo.

- **Paso 1.** Haz clic [aquí](https://app.roboflow.com/login) para registrarte en una cuenta de Roboflow.

- **Paso 2.** Haz clic en **Create New Project** para iniciar tu proyecto.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/YOLOV5/2.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/2.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 3.** Rellena el campo **Project Name**, mantén por defecto la **Licencia (CC BY 4.0)** y el **Tipo de proyecto (Object Detection (Bounding Box))**. En la columna **What will your model predict?**, escribe el nombre del grupo de anotación. Por ejemplo, en nuestro caso elegimos **apples**. Este nombre debe representar todas las clases de tu conjunto de datos. Finalmente, haz clic en **Create Public Project**.

<!-- <div align=center><img width=350 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/6.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/6.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 4.** Arrastra y suelta las imágenes que hayas capturado usando el módulo Grove - Vision AI.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/7.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/7.png" alt="pir" width={600} height="auto" /></p>

- **Paso 5.** Una vez que las imágenes hayan sido procesadas, haz clic en **Finish Uploading**. Espera pacientemente hasta que todas las imágenes se carguen.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/4.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/4.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 6.** Después de cargar las imágenes, haz clic en **Assign Images**.

<!-- <div align=center><img width=300 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/5.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/5.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 7.** Selecciona una imagen, dibuja un recuadro alrededor de una manzana, elige la etiqueta **apple** y presiona **ENTER**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/9.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/9.png" alt="pir" width={600} height="auto" /></p>

- **Paso 8.** Repite el mismo proceso para las demás manzanas.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/10.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/10.png" alt="pir" width={600} height="auto" /></p>

**Nota:** Intenta etiquetar todas las manzanas que aparezcan en la imagen. Incluso si solo se ve parcialmente una manzana, también debes etiquetarla.

- **Paso 9.** Continúa anotando todas las imágenes del conjunto de datos.

Roboflow cuenta con una función llamada **Label Assist**, que puede predecir las etiquetas automáticamente para agilizar el proceso de anotación. Sin embargo, solo funciona con ciertos tipos de objetos. Para activarla, simplemente presiona el botón **Label Assist**, selecciona un **modelo**, selecciona las **clases** y navega entre las imágenes para ver las etiquetas con las cajas delimitadoras ya generadas.

<!-- <div align=center><img width=200 src="https://files.seeedstudio.com/wiki/YOLOV5/41.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/41.png" alt="pir" width={600} height="auto" /></p>

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/39.png" alt="pir" width={600} height="auto" /></p>

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/40.png" alt="pir" width={600} height="auto" /></p>
Como se puede observar, esta función solo ayuda a predecir anotaciones para las 80 clases predefinidas. Si tus imágenes no contienen esas clases, no podrás usar esta función.

- **Paso 10.** Una vez que hayas terminado la anotación, haz clic en **Add images to Dataset**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/YOLOV5/25.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/25.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 11.** Luego dividiremos las imágenes en "Train, Valid y Test". Mantén los porcentajes predeterminados de distribución y haz clic en **Add Images**.

<!-- <div align=center><img width=330 src="https://files.seeedstudio.com/wiki/YOLOV5/26.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/26.png" alt="pir" width={600} height="auto" /></p>

- **Paso 12.** Haz clic en **Generate New Version**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/YOLOV5/27.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/27.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 13.** Ahora puedes aplicar **Preprocesamiento** y **Aumentos (Augmentation)** si lo deseas. Aquí cambiaremos la opción de **Resize** a **192x192**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/11.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/11.png" alt="pir" width={600} height="auto" /></p>

<!-- <div align=center><img width=450 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/13.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/13.png" alt="pir" width={600} height="auto" /></p>

Cambiar el tamaño de imagen a 192x192 nos permite usar ese tamaño durante el entrenamiento, lo cual hace que el proceso sea más rápido. De lo contrario, durante el entrenamiento todas las imágenes tendrían que ser convertidas, lo que consume más recursos de CPU y ralentiza el proceso.

- **Paso 14.** A continuación, continúa con las opciones predeterminadas y haz clic en **Generate**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/14.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/14.png" alt="pir" width={600} height="auto" /></p>

- **Paso 15.** Haz clic en **Export**, selecciona el **Formato** como **YOLO v5 PyTorch**, selecciona **show download code** y haz clic en **Continue**.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/54.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/54.png" alt="pir" width={600} height="auto" /></p>
Esto generará un fragmento de código que usaremos más adelante dentro del entrenamiento en Google Colab. Por lo tanto, mantén esta ventana abierta en segundo plano.

<!-- <div align=center><img width=600 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/55.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/55.png" alt="pir" width={600} height="auto" /></p>

### Entrenar usando YOLOv5 en Google Colab

Una vez que hemos terminado de anotar el conjunto de datos, necesitamos entrenarlo. Dirígete a [esta sección](https://wiki.seeedstudio.com/Vision_AI_with_Customizable_Models/#train-using-yolov5-on-google-colab) donde se explica cómo entrenar un modelo de IA usando YOLOv5 ejecutándose en Google Colab.

## <span id="jump3">3. Implementar el modelo entrenado y realizar inferencias</span>

### Módulo Grove - Vision AI

Ahora moveremos el archivo **model-1.uf2** que obtuvimos al final del entrenamiento hacia el módulo Grove - Vision AI. Aquí conectaremos el Grove - Vision AI Module con el [Wio Terminal](https://www.seeedstudio.com/Wio-Terminal-p-4509.html) para visualizar los resultados de la inferencia.

**Nota:** Si es la primera vez que usas Arduino, te recomendamos consultar [Getting Started with Arduino](https://wiki.seeedstudio.com/Getting_Started_with_Arduino). También debes seguir [esta guía](https://wiki.seeedstudio.com/Wio-Terminal-Getting-Started/#getting-started) para configurar el Wio Terminal con el IDE de Arduino.

- **Paso 1.** Instala la última versión de [Google Chrome](https://www.google.com/chrome) o [Microsoft Edge](https://www.microsoft.com/en-us/edge?r=1) y ábrelo.

- **Paso 2.** Conecta el módulo Grove - Vision AI a tu PC mediante un cable USB tipo C.

<!-- <div align=center><img width=450 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/47.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/47.png" alt="pir" width={600} height="auto" /></p>

- **Paso 3.** Haz doble clic en el botón de arranque (boot) del módulo Grove - Vision AI para entrar en el modo de almacenamiento masivo.

<!-- <div align=center><img width=220 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/48.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/48.png" alt="pir" width={600} height="auto" /></p>

Después de esto, verás una nueva unidad de almacenamiento en tu explorador de archivos con el nombre **GROVEAI**.

<!-- <div align=center><img width=280 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/19.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/19.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 4.** Arrastra y suelta el archivo **model-1.uf2** dentro de la unidad **GROVEAI**

Tan pronto como se termine de copiar el archivo uf2 en la unidad, esta desaparecerá. Esto significa que el archivo se ha cargado exitosamente en el módulo.

**Nota:** Si tienes 4 archivos de modelo listos, puedes cargarlos uno por uno. Primero arrastra un modelo, espera a que termine de copiarse, entra nuevamente en modo boot, y repite el proceso con los modelos restantes.

- **Paso 5.** Mientras el módulo Grove - Vision AI aún está conectado al PC mediante USB, conéctalo al Wio Terminal usando el puerto Grove I2C de la siguiente forma:

<!-- <div align=center><img width=250 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/49.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/49.png" alt="pir" width={600} height="auto" /></p>

- **Paso 6.** Instala la [librería Seeed_Arduino_GroveAI](https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI) en el IDE de Arduino y abre el ejemplo **object_detection.ino**

- **Paso 7.** Si solo has cargado un modelo (con índice 1) en el módulo Grove - Vision AI, se cargará automáticamente. Sin embargo, si has cargado múltiples modelos, puedes [especificar cuál usar](https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L12) cambiando la línea a **MODEL_EXT_INDEX_[valor]**, donde el valor puede ser 1, 2, 3 o 4.

```cpp
// for example:
if (ai.begin(ALGO_OBJECT_DETECTION, MODEL_EXT_INDEX_2))
```

Lo anterior cargará el modelo con índice 2

- **Paso 8.** Como estamos detectando manzanas, haremos un pequeño cambio en el código [aquí](https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L55)

```cpp
Serial.print("Number of apples: ");
```

- **Paso 9.** Conecta el Wio Terminal a la PC, sube este código al Wio Terminal y abre el monitor serial del Arduino IDE con una velocidad de 115200 baudios.

<!-- <div align=center><img width=500 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/42.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/42.png" alt="pir" width={600} height="auto" /></p>
Podrás ver la información de detección en el monitor serial como se muestra arriba.

- **Paso 10.** [Haz clic aquí](https://files.seeedstudio.com/grove_ai_vision/index.html) para abrir una ventana de vista previa del stream de la cámara con las detecciones

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/31.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/31.png" alt="pir" width={600} height="auto" /></p>

- **Paso 11.** Haz clic en el botón **Connect**. Luego aparecerá una ventana emergente en el navegador. Selecciona **Grove AI - Paired** y haz clic en **Connect**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/32.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/32.png" alt="pir" width={600} height="auto" /></p>

- **Paso 12.** ¡Visualiza resultados de inferencia en tiempo real usando la ventana de vista previa!

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/33.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/33.jpg" alt="pir" width={600} height="auto" /></p>

Como puedes ver arriba, las manzanas están siendo detectadas con cuadros delimitadores a su alrededor. Aquí el "0" corresponde a cada detección de la misma clase. Si tienes múltiples clases, se nombrarán como 0,1,2,3,4 y así sucesivamente. ¡También se muestra la puntuación de confianza para cada manzana detectada (0.8 y 0.84 en la demo anterior)!

## Contenido adicional

Si te sientes más aventurero, ¡puedes continuar siguiendo el resto de la wiki!

### ¿Puedo entrenar un modelo de IA en mi PC?

También puedes usar tu propia PC para entrenar un modelo de detección de objetos. Sin embargo, el rendimiento del entrenamiento dependerá del hardware que tengas. Además, necesitas una PC con sistema operativo Linux para el entrenamiento. En esta wiki hemos usado una PC con Ubuntu 20.04.

- **Paso 1.** Clona el repositorio **yolov5-swift** e instala los requisitos de **requirements.txt** en un entorno de **Python>=3.7.0**

```sh
git clone https://github.com/Seeed-Studio/yolov5-swift 
cd yolov5-swift
pip install -r requirements.txt
```

- **Paso 2.** Si seguiste los pasos de esta wiki antes, recordarás que exportamos el dataset después de anotarlo en Roboflow. También en Roboflow Universe descargamos el dataset. En ambos métodos apareció una ventana como la siguiente donde se pregunta en qué formato descargar el dataset. Ahora, selecciona **download zip to computer**, en **Format** elige **YOLO v5 PyTorch** y haz clic en **Continue**

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/16.png" alt="pir" width={600} height="auto" /></p>

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/17.png" alt="pir" width={600} height="auto" /></p>

Después de esto, se descargará un archivo **.zip** en tu computadora.

- **Paso 3.** Copia y pega el archivo .zip que descargaste dentro del directorio **yolov5-swift** y extráelo.

```sh
# example
cp ~/Downloads/Apples.v1i.yolov5pytorch.zip ~/yolov5-swift
unzip Apples.v1i.yolov5pytorch.zip
```

- **Paso 4.** Abre el archivo **data.yaml** y edita los directorios **train** y **val** como se indica.

```sh
train: train/images
val: valid/images
```

- **Paso 5.** Descarga un modelo preentrenado adecuado para nuestro entrenamiento.

```sh
sudo apt install wget
wget https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt
```

- **Paso 6.** Ejecuta el siguiente comando para comenzar el entrenamiento

Aquí podemos pasar varios argumentos:

- **img:** define el tamaño de la imagen de entrada
- **batch:** determina el tamaño del lote (batch)
- **epochs:** define el número de épocas de entrenamiento
- **data:** establece la ruta al archivo yaml
- **cfg:** especifica la configuración del modelo
- **weights:** especifica una ruta personalizada para los pesos
- **name:** nombre para los resultados
- **nosave:** solo guarda el checkpoint final
- **cache:** almacena en caché las imágenes para un entrenamiento más rápido

```sh
python3 train.py --img 192 --batch 64 --epochs 100 --data data.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name yolov5n6_results --cache
```

Para un dataset de detección de manzanas con 987 imágenes, el proceso de entrenamiento tardó alrededor de 30 minutos en una PC local con GPU NVIDIA GeForce GTX 1660 Super con 6GB de memoria GPU.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/44.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/44.png" alt="pir" width={600} height="auto" /></p>
Si seguiste el proyecto de Colab anterior, sabes que puedes cargar 4 modelos en el dispositivo al mismo tiempo. Sin embargo, ten en cuenta que solo un modelo puede estar activo a la vez. Esto lo puede especificar el usuario y se explicará más adelante en esta wiki.

- **Paso 7.** Si navegas a `runs/train/exp/weights`, verás un archivo llamado **best.pt**. Este es el modelo generado tras el entrenamiento.

<!-- <div align=center><img width=600 src="https://files.seeedstudio.com/wiki/YOLOV5/33.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/33.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 8.** Exporta el modelo entrenado a TensorFlow Lite

```sh
python3 export.py --data {dataset.location}/data.yaml --weights runs/train/yolov5n6_results/weights/best.pt --imgsz 192 --int8 --include tflite  
```

- **Paso 9.** Convierte TensorFlow Lite a un archivo UF2

UF2 es un formato de archivo desarrollado por Microsoft. Seeed utiliza este formato para convertir archivos .tflite a .uf2, permitiendo almacenar archivos tflite en dispositivos AIoT lanzados por Seeed. Actualmente los dispositivos de Seeed soportan hasta 4 modelos, cada modelo (.tflite) debe ser menor a 1MB.

Puedes especificar el índice donde colocar el modelo con el parámetro -t.

Por ejemplo:

- `-t 1`: index 1
- `-t 2`: index 2

```sh
# Place the model to index 1
python3 uf2conv.py -f GROVEAI -t 1 -c runs//train/yolov5n6_results//weights/best-int8.tflite -o model-1.uf2
```

Aunque puedes cargar 4 modelos en el dispositivo a la vez, recuerda que solo uno puede estar activo a la vez. Esto puede ser especificado por el usuario y se explicará más adelante en esta wiki.

- **Paso 10.** Ahora se generará un archivo llamado **model-1.uf2**. ¡Este es el archivo que cargaremos en el Grove - Vision AI Module para realizar la inferencia!

## Recursos

- **[Página Web]** [Documentación de YOLOv5](https://docs.ultralytics.com)

- **[Página Web]** [Ultralytics HUB](https://ultralytics.com/hub)

- **[Página Web]** [Documentación de Roboflow](https://docs.roboflow.com)

- **[Página Web]** [Documentación de TensorFlow Lite](https://www.tensorflow.org/lite/guide)

## Soporte Técnico y Discusión de Producto

¡Gracias por elegir nuestros productos! Estamos aquí para brindarte diferentes tipos de soporte y asegurar que tu experiencia con nuestros productos sea lo más fluida posible. Ofrecemos varios canales de comunicación para atender diferentes preferencias y necesidades.

<div class="button_tech_support_container">
<a href="https://forum.seeedstudio.com/" class="button_forum"></a> 
<a href="https://www.seeedstudio.com/contacts" class="button_email"></a>
</div>

<div class="button_tech_support_container">
<a href="https://discord.gg/eWkprNDMU7" class="button_discord"></a> 
<a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" class="button_discussion"></a>
</div>
