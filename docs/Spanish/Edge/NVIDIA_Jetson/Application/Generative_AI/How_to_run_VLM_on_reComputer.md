---
description: Esta wiki proporciona un tutorial sobre c√≥mo ejecutar VLM en una reComputer Jetson.
title: Como ejecutar un VLM en una reComputer
keywords:
- reComputer
- VLM
image: https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png
slug: /es/run_vlm_on_recomputer
last_update:
  date: 7/24/2024
  author: ZhuYaoHui
---

# C√≥mo ejecutar un VLM en una reComputer con Jetson Platform Services

## Introducci√≥n
Los modelos de lenguaje de visi√≥n (VLM) son modelos multimodales que admiten im√°genes, videos y texto y utilizan una combinaci√≥n de LLM's y transformadores de visi√≥n. Con base en esta capacidad, pueden admitir indicaciones de texto para consultar videos e im√°genes, lo que permite chatear con el video y definir alertas basadas en lenguaje natural. El [servicio VLM AI](https://docs.nvidia.com/jetson/jps/inference-services/vlm.html) permite una implementaci√≥n r√°pida de VLM con Jetson Platform Services para aplicaciones de informaci√≥n de video. El servicio VLM expone puntos finales de API REST para configurar la entrada de la transmisi√≥n de video, establecer alertas y hacer preguntas en lenguaje natural sobre la transmisi√≥n de video de entrada.

Esta wiki proporciona un tutorial sobre c√≥mo ejecutar un VLM en la [reComputer J4012 Jetson Orin NX](https://www.seeedstudio.com/reComputer-J4012-p-5586.html).

<div align="center">
    <img width={900} 
     src="https://files.seeedstudio.com/wiki/reComputer/Application/vlm/vlmgif.gif" />
</div>

## Requerimientos
Antes de continuar con el proceso de configuraci√≥n, aseg√∫rate de que tu sistema cumpla con los siguientes prerequisitos:

<div align="center">
    <img width={800} 
     src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_J4012.png" />
</div>

<div class="get_one_now_container" style={{textAlign: 'center'}}>
    <a class="get_one_now_item" href="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_J4012.png">
      <strong><span><font color={'FFFFFF'} size={"4"}> Conseguir una üñ±Ô∏è</font></span></strong>
    </a>
</div>

- Una reComputer J4012 Orin NX 16G con Ubuntu `22.04` o `posterior`.
- Versi√≥n de controlador: `535.113.01`, Jetpack `6.0` y CUDA Versi√≥n: `12.2`.
- Aseg√∫rate de que JetPack y los paquetes de servicios Jetson relacionados est√©n instalados.
  ```bash
  sudo apt-get install nvidia-jetpack
  sudo apt install nvidia-jetson-services
  ```
- Se pueden transmitir c√°maras IP o v√≠deos locales a trav√©s de RTSP. (Recomendamos utilizar nuestro [tutorial de NVStreamer](/getting_started_with_nvstreamer) proporcionado para la transmisi√≥n RTSP).




## Primeros pasos

**Paso 1**: Descarga el paquete de aplicaci√≥n **`vlm-1.1.0.tar.gz`** de NGC en tu Jetson usando este enlace: [Flujo de trabajo y recursos de referencia de NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/jps/resources/reference-workflow-and-resources). Deber√°s ingresar tus credenciales de NGC. En la p√°gina, usa una de las opciones disponibles en el men√∫ **`Descargar`** (esquina superior derecha):
```bash
tar -xvf vlm-1.1.0.tar.gz
cd ~/vlm/example_1
```

**Paso 2**: El servicio VLM AI utilizar√° los servicios `jetson-ingress` y `jetson-monitoring`. Debes configurar estos dos servicios para integrarlos con el servicio VLM AI. Copia la configuraci√≥n predeterminada proporcionada en el directorio de configuraci√≥n del servicio correspondiente:
```bash
sudo cp config/vlm-nginx.conf /opt/nvidia/jetson/services/ingress/config
sudo cp config/prometheus.yml /opt/nvidia/jetson/services/monitoring/config/prometheus.yml
sudo cp config/rules.yml /opt/nvidia/jetson/services/monitoring/config/rules.yml
```

**Paso 3**: Ejecuta los servicios b√°sicos:
```bash
sudo systemctl start jetson-ingress
sudo systemctl start jetson-monitoring
sudo systemctl start jetson-sys-monitoring
sudo systemctl start jetson-gpu-monitoring
sudo systemctl start jetson-redis
sudo systemctl start jetson-vst
```

**Paso 4**: Al iniciar el servicio VLM por primera vez, descargar√° y cuantificar√° autom√°ticamente el VLM. Este proceso puede llevar alg√∫n tiempo. Si realizas la implementaci√≥n en Orin NX16, es posible que necesites montar m√°s espacio SWAP porque el proceso de cuantificaci√≥n puede consumir una gran cantidad de memoria. Ejecuta los siguientes comandos para montar m√°s espacio SWAP:

```bash
sudo fallocate -l 10G /data/10GB.swap
sudo mkswap /data/10GB.swap
sudo swapon /data/10GB.swap
```

**Paso 5**: Inicia el servicio VLM AI:
```bash
cd ~/vlm/example_1
sudo docker compose up -d
```
Para comprobar si se han iniciado todos los contenedores necesarios, puedes ejecutar el siguiente comando:
```bash
sudo docker ps
```
<div align="center">
    <img width={1000} 
     src="https://files.seeedstudio.com/wiki/reComputer/Application/vlm/vlmfig2.png" />
</div>

## Agrega la entrada de flujo RTSP
Primero puedes agregar una secuencia RTSP para que el modelo VLM la use con el siguiente comando curl.
Se recomienda utilizar el [tutorial de NVStreamer](/getting_started_with_nvstreamer) para la transmisi√≥n.
- **Paso 1**: Reemplaza `0.0.0.0` con el IP de tu Jetson y el enlace `liveStreamUrl` con tu enlace RTSP, luego ingresa el siguiente comando en la terminal:
    ```bash
    curl --location 'http://0.0.0.0:5010/api/v1/live-stream' \
    --header 'Content-Type: application/json' \
    --data '{
    "liveStreamUrl": "rtsp://0.0.0.0:31554/nvstream/root/store/nvstreamer_videos/car.mp4"
    }'
    ```
    Nota: Adem√°s del comando curl, tambi√©n puedes probar directamente la API REST a trav√©s de la p√°gina de documentaci√≥n de la API, que est√° disponible en `http://0.0.0.0:5010/docs` cuando se inicia el servicio de detecci√≥n de disparo cero.

- **Paso 2**: Despu√©s de ejecutar el primer paso, se devolver√° una identificaci√≥n. Debes registrar esta identificaci√≥n para usarla en los pasos siguientes:
    ```bash
    {"id": "a782e200-eb48-4d17-a1b9-5ac0696217f7"}
    ```
    Tambi√©n puedes obtener la identificaci√≥n m√°s tarde usando el siguiente comando:

    ```bash
    curl --location 'http://0.0.0.0:5010/api/v1/live-stream'
    ```
    Para eliminar una secuencia por su ID, puedes utilizar el siguiente comando:
    ```bash
    curl --location --request DELETE 'http://0.0.0.0:5010/api/v1/live-stream/{id}'
    ```

## Configurar alertas
Las alertas son preguntas que el VLM evaluar√° continuamente en la entrada de la transmisi√≥n en vivo. Para cada conjunto de reglas de alerta, el VLM intentar√° decidir si es Verdadero o Falso en funci√≥n del fotograma m√°s reciente de la transmisi√≥n en vivo. Estos estados Verdadero y Falso, seg√∫n lo determina el VLM, se env√≠an a un websocket y al servicio de monitoreo Jetson.

Al configurar alertas, la regla de alerta debe formularse como una pregunta de s√≠ o no. Como "¬øHay fuego?" o ‚Äú¬øHay humo?‚Äù. El cuerpo de la solicitud tambi√©n debe tener el campo "id" que corresponde al ID de la transmisi√≥n que se devolvi√≥ cuando se agreg√≥ la transmisi√≥n RTSP.

De forma predeterminada, el servicio VLM admite hasta 10 reglas de alerta. Esto se puede aumentar ajustando los archivos de configuraci√≥n.

**Paso 1**: Reemplaz `0.0.0.0` con la direcci√≥n IP de tu reComputer, modifica `alerts` para incluir los objetos que necesitas para las alertas, usa el `id` devuelto en el paso anterior. Despu√©s de completar el comando, ingresa lo siguiente en la terminal:
``` bash
curl --location 'http://0.0.0.0:5010/api/v1/alerts' \
--header 'Content-Type: application/json' \
--data '{
    "alerts": ["is there a fire?", "is there smoke?"],
    "id": "a782e200-eb48-4d17-a1b9-5ac0696217f7"
}'
```

## Ver el resultado de la transmisi√≥n RTSP
El resultado de la detecci√≥n se transmitir√° a trav√©s de `rtsp://reComputer_ip:5011/out`. Proporcionamos una secuencia de comandos de Python para visualizar la salida de la transmisi√≥n de video. Debes instalar la biblioteca opencv-python con anticipaci√≥n y luego ejecutar la siguiente secuencia de comandos de Python:
- **Paso 1:** Instala la biblioteca opencv-python:
    ```bash
    pip install opencv-python
    ```
- **Paso 2:** Ejecuta el siguiente script de Python:
    ```python
    import cv2
    rtsp_url = "rtsp://reComputer_ip:5011/out"
    cap = cv2.VideoCapture(rtsp_url)
    if not cap.isOpened():
        print("Cannot open RTSP stream")
        exit()
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Failed to retrieve frame")
            break
        cv2.imshow('RTSP Stream', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cap.release()
    cv2.destroyAllWindows()
    ```

## Apagado
Para detener el servicio de detecci√≥n de disparo cero, ejecuta el siguiente comando en el mismo directorio donde se encuentra el archivo `compose.yaml`:
```bash
sudo docker compose down
```

## M√°s detalles
Modelos de lenguaje visual (VLM) con Jetson Platform Services: https://docs.nvidia.com/jetson/jps/inference-services/vlm.html

## Soporte Tech y discusi√≥n del producto

¬°Gracias por elegir nuestros productos! Estamos aqu√≠ para darte soporte y asegurar que tu experiencia con nuestros productos sea la mejor posible. Tenemos diversos canales de comunicaci√≥n para adaptarnos distintas preferencias y necesidades.

<div class="button_tech_support_container">
<a href="https://forum.seeedstudio.com/" class="button_forum"></a> 
<a href="https://www.seeedstudio.com/contacts" class="button_email"></a>
</div>

<div class="button_tech_support_container">
<a href="https://discord.gg/eWkprNDMU7" class="button_discord"></a> 
<a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" class="button_discussion"></a>
</div>