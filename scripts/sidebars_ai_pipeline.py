#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€é”®ç”Ÿæˆå¤šè¯­è¨€ sidebarï¼ˆåªé€‰è¯­è¨€å³å¯ï¼‰
- ä» ProductSidebar å¤åˆ¶ç»“æ„
- è‡ªåŠ¨ç”¨ Claude ç¿»è¯‘ labelï¼ˆæ˜¯å¦ç¿»è¯‘ dirName å¯åœ¨é¢„è®¾é‡Œæ§åˆ¶ï¼‰
- æ”¹å†™æ‰€æœ‰æ–‡æ¡£å¼•ç”¨ï¼šid / items ä¸­è£¸å­—ç¬¦ä¸² / link.id
- å†™å›ç›®æ ‡ Sidebar é”®ï¼ˆCNSidebar/jaSidebar/esSidebarï¼‰
- è¯»å– .env ä¸­çš„ ANTHROPIC_API_KEY

æœ€ç®€ç”¨æ³•ï¼ˆåªé€‰è¯­è¨€ï¼‰ï¼š
  python scripts/sidebars_ai_pipeline.py --lang es

å¯é€‰å‚æ•°ï¼š
  --sidebars        sidebars.js è·¯å¾„ï¼ˆé»˜è®¤ ./sidebars.jsï¼‰
  --product-key     è‹±æ–‡æºé”®åï¼ˆé»˜è®¤ ProductSidebarï¼‰
  --model           Claude æ¨¡å‹ï¼ˆé»˜è®¤ claude-sonnet-4-20250514ï¼‰
  --dry-run         åªæ‰“å°ä¿®æ”¹æ‘˜è¦ï¼Œä¸å†™ç›˜
"""

import os, re, json, time, argparse, random
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set

# ========== è¯­è¨€é¢„è®¾ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ==========
# - target_key: ç›®æ ‡ Sidebar é”®å
# - folder: è¯­è¨€ç›®å½•å‰ç¼€ï¼ˆä¼šæ”¾åœ¨è·¯å¾„æœ€å‰é¢ï¼‰
# - fileprefix: æ–‡ä»¶åå‰ç¼€ï¼ˆä¼šåŠ åœ¨æœ€åä¸€æ®µæ–‡ä»¶åä¹‹å‰ï¼‰
# - translate_dirname: æ˜¯å¦ä¹Ÿç¿»è¯‘ dirNameï¼ˆå¤šæ•°æƒ…å†µä¸‹ False æ›´ç¨³ï¼‰
LANG_PRESETS = {
    "zh": {"target_key": "CNSidebar", "folder": "zh-CN/", "fileprefix": "cn_", "translate_dirname": False},
    "ja": {"target_key": "jaSidebar","folder": "ja/", "fileprefix": "ja_", "translate_dirname": False},
    "es": {"target_key": "esSidebar","folder": "es/", "fileprefix": "es_", "translate_dirname": False},
}

ANTHROPIC_MODEL_DEFAULT = "claude-sonnet-4-20250514"

# ========== .env è¯»å– ==========
def load_env():
    # ä¼˜å…ˆ python-dotenv
    try:
        from dotenv import load_dotenv
        load_dotenv()
        return
    except Exception:
        pass
    # ç®€æ˜“è§£æ .envï¼ˆä»…æ”¯æŒ KEY=VALUEï¼‰
    env_path = Path(".env")
    if env_path.exists():
        for line in env_path.read_text(encoding="utf-8").splitlines():
            line=line.strip()
            if not line or line.startswith("#") or "=" not in line: continue
            k,v=line.split("=",1)
            k=k.strip(); v=v.strip().strip('"').strip("'")
            if k and (k not in os.environ):
                os.environ[k]=v

# ========== Claude å®¢æˆ·ç«¯ ==========
def _make_claude_client():
    api_key = os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("ANTHROPIC_APIKEY")
    if not api_key:
        raise RuntimeError("æœªæ£€æµ‹åˆ° ANTHROPIC_API_KEYï¼ˆè¯·åœ¨ .env ä¸­é…ç½®ï¼‰ã€‚")
    try:
        from anthropic import Anthropic
        return ("sdk", Anthropic(api_key=api_key))
    except Exception:
        try:
            import requests  # noqa
        except Exception:
            raise RuntimeError("æœªå®‰è£… anthropic SDKï¼Œä¸”ç¼ºå°‘ requestsã€‚è¯· `pip install anthropic requests python-dotenv`")
        return ("http", api_key)
    
def _strip_code_fences(s: str) -> str:
    s = s.strip()
    # å»æ‰ ```json / ``` ä»£ç å—å›´æ ï¼ˆé¦–å°¾å„ä¸€è¡Œï¼‰
    if s.startswith("```"):
        s = re.sub(r'^```[a-zA-Z0-9_-]*\r?\n', '', s, flags=re.MULTILINE)
        s = re.sub(r'\r?\n```$', '', s, flags=re.MULTILINE)
    return s.strip()

def _extract_balanced_json(s: str) -> Optional[str]:
    """
    ä»ä»»æ„æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªâ€œé…å¯¹å®Œæ•´çš„å¤§æ‹¬å·â€JSON å¯¹è±¡å­ä¸²ã€‚
    """
    start = s.find('{')
    if start == -1:
        return None
    depth = 0
    for i in range(start, len(s)):
        ch = s[i]
        if ch == '{':
            depth += 1
        elif ch == '}':
            depth -= 1
            if depth == 0:
                return s[start:i+1]
    return None  # æ‰¾ä¸åˆ°é—­åˆ

def _strip_comments_preserve_tokens(js: str) -> Tuple[str, Dict[str, str]]:
    """
    æŠŠ JS å•è¡Œ // ä¸å¤šè¡Œ /* */ æ³¨é‡Šæ›¿æ¢æˆå ä½ç¬¦ï¼Œè¿”å› (å»æ³¨é‡Šæ–‡æœ¬, å ä½æ˜ å°„)ã€‚
    ä¼šè·³è¿‡å­—ç¬¦ä¸²å†…å®¹ï¼Œé¿å…æŠŠå­—ç¬¦ä¸²é‡Œçš„ // æˆ– /* å½“æˆæ³¨é‡Šã€‚
    """
    tokens: Dict[str, str] = {}
    out = []
    i = 0
    n = len(js)
    cidx = 0

    def new_token(text: str) -> str:
        nonlocal cidx
        tok = f"__COMMENT__{cidx}__"
        cidx += 1
        tokens[tok] = text
        return tok

    while i < n:
        ch = js[i]
        # å­—ç¬¦ä¸²ï¼š'...' æˆ– "..."
        if ch in ("'", '"'):
            start = i
            i = _skip_string(js, i)
            # _skip_string è¿”å›çš„æ˜¯å¼•å·ä½ç½®ï¼Œéœ€åŒ…å«è¯¥å¼•å·
            out.append(js[start:i+1])
            i += 1
            continue

        # å•è¡Œæ³¨é‡Š //
        if ch == "/" and i + 1 < n and js[i+1] == "/":
            j = js.find("\n", i)
            if j == -1:
                j = n
            comment_text = js[i:j]
            out.append(new_token(comment_text))
            i = j
            continue

        # å¤šè¡Œæ³¨é‡Š /* ... */
        if ch == "/" and i + 1 < n and js[i+1] == "*":
            j = js.find("*/", i + 2)
            if j == -1:
                j = n - 2
            comment_text = js[i:j+2]
            out.append(new_token(comment_text))
            i = j + 2
            continue

        out.append(ch)
        i += 1

    return "".join(out), tokens


def _restore_comments_from_tokens(js: str, tokens: Dict[str, str]) -> str:
    if not tokens:
        return js
    # ç”¨æœ€ç®€å•å®‰å…¨çš„æ›¿æ¢ï¼ˆtoken ä¸ä¼šç›¸äº’åŒ…å«ï¼‰
    for tok, text in tokens.items():
        js = js.replace(tok, text)
    return js

def claude_translate(values: List[str], target_lang: str, what: str, model: str, never_translate: Optional[List[str]] = None) -> Dict[str, str]:
    if not values:
        return {}

    mode, client_or_key = _make_claude_client()

    # ===== ä¿æŠ¤ä¸“æœ‰åè¯ / æ ‡è¯†ç¬¦ =====
    glossary = ""
    if never_translate:
        glossary = "Never translate these terms (keep exactly as-is): " + ", ".join(sorted(set(never_translate))) + "\n"

    sys_rules = (
        "You are translating UI strings for a documentation sidebar.\n"
        "Return ONLY a raw JSON object mapping original->translation. No markdown code fences. No prose.\n"
        "Rules:\n"
        "- Translate visible natural-language words ONLY.\n"
        "- DO NOT translate proper nouns, product/brand names, or glossary terms.\n"
        "- Preserve code identifiers, CamelCase/PascalCase, ALL-CAPS tokens, numbers, slashes '/', hyphens '-', underscores '_', version tokens.\n"
        "- If unsure whether a token is a proper noun or identifier, KEEP IT UNCHANGED.\n"
        "- No comments, no trailing commas, no extra keys.\n"
        "- Keep leading/trailing spaces unchanged.\n"
    ) + glossary

    prompt_map = {
        "zh": f"å°†ä»¥ä¸‹ {what} åˆ—è¡¨ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ã€‚ä»…ç¿»è¯‘å¯è§è‡ªç„¶è¯­è¨€ï¼Œä¿ç•™ä¸“æœ‰åè¯/äº§å“å/è·¯å¾„/æ ‡è¯†ç¬¦ï¼›åªè¿”å›**è£¸ JSON**ï¼š",
        "ja": f"æ¬¡ã®{what}ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å¯è¦–ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ç¿»è¨³ã—ã€å›ºæœ‰åè©ãƒ»è£½å“åãƒ»ãƒ‘ã‚¹ãƒ»è­˜åˆ¥å­ã¯ä¿æŒã€‚**ç”Ÿã®JSON**ã®ã¿è¿”ã—ã¦ãã ã•ã„ï¼š",
        "es": f"Traduce la lista de {what} al espaÃ±ol. Traduce solo texto visible; conserva nombres propios/marcas/rutas/identificadores. Devuelve **solo JSON en crudo**:",
    }
    user_inst = prompt_map.get(
        target_lang,
        f"Translate {what} values to {target_lang}. Translate visible text only; keep proper nouns/brands/paths/identifiers unchanged. Return **only raw JSON**, no markdown, no prose:"
    )
    payload = json.dumps(values, ensure_ascii=False, indent=2)
    user_content = user_inst + "\n" + payload

    # ===== å†…éƒ¨ä¸€ä¸ªå°é‡è¯•å™¨ï¼ˆç”¨äº 5xx/ç½‘ç»œé”™è¯¯ï¼‰
    def _post_with_http():
        import requests
        url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": client_or_key if mode == "http" else os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("ANTHROPIC_APIKEY",""),
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }
        data = {
            "model": model,
            "max_tokens": 4000,      # â† åˆç†ä¸Šé™
            "temperature": 0.2,      # â† ä¸ SDK å¯¹é½
            "system": sys_rules,
            "messages": [{"role": "user", "content": user_content}],
        }
        last_err = None
        for attempt in range(4):  # 0,1,2,3
            try:
                r = requests.post(url, headers=headers, data=json.dumps(data), timeout=60)
                if r.status_code >= 500:
                    raise RuntimeError(f"HTTP {r.status_code}: {r.text[:200]}")
                if r.status_code >= 400:
                    # 4xx ç›´æ¥æŠ›
                    raise RuntimeError(f"Claude API é”™è¯¯ï¼š{r.status_code} {r.text[:200]}")
                j = r.json()
                blocks = j.get("content", [])
                return (blocks[0].get("text", "") if blocks else "") or j.get("text", "")
            except Exception as e:
                last_err = e
                if attempt < 3:
                    delay = (0.6 * (2 ** attempt)) + random.uniform(0, 0.3)
                    print(f"[warn] HTTP call failed ({e}); retry in {delay:.2f}s")
                    time.sleep(delay)
                else:
                    raise last_err

    text = ""
    if mode == "sdk":
        # å…ˆè¯• SDKï¼›å¤±è´¥å†å›é€€ HTTP
        try:
            from anthropic import InternalServerError
        except Exception:
            InternalServerError = tuple()  # å“‘å…ƒ

        try:
            cli = client_or_key
            # SDK ä¹ŸåŠ ä¸€ç‚¹é‡è¯•ï¼ˆåªå¯¹ 5xx/ç½‘ç»œï¼‰
            for attempt in range(3):
                try:
                    resp = cli.messages.create(
                        model=model,
                        max_tokens=4000,
                        temperature=0.2,
                        system=sys_rules,
                        messages=[{"role": "user", "content": user_content}],
                    )
                    text = resp.content[0].text if getattr(resp, "content", None) else ""
                    break
                except Exception as e:
                    msg = str(e)
                    retriable = isinstance(e, InternalServerError) or "upstream" in msg or "timeout" in msg.lower()
                    if attempt < 2 and retriable:
                        delay = (0.6 * (2 ** attempt)) + random.uniform(0, 0.3)
                        print(f"[warn] SDK call failed ({e}); retry in {delay:.2f}s")
                        time.sleep(delay)
                        continue
                    # SDK å…œåº•è½¬ HTTP
                    print("[info] Falling back to HTTP client...")
                    text = _post_with_http()
                    break
        except Exception:
            # SDK å¤±è´¥åå†è¯• HTTPï¼ˆå«é‡è¯•ï¼‰
            print("[info] SDK unavailable; using HTTP client.")
            text = _post_with_http()
    else:
        # çº¯ HTTP æ¨¡å¼ï¼ˆå«é‡è¯•ï¼‰
        text = _post_with_http()

    # ===== è§£æï¼šå»å›´æ  -> æ‰¾é…å¯¹å¤§æ‹¬å· -> å»å°¾é€—å· -> json.loads =====
    raw = _strip_code_fences(text)
    cand = _extract_balanced_json(raw) or _extract_balanced_json(text)
    if not cand:
        raise RuntimeError(f"Claude è¿”å›é JSONï¼š{text[:200]}")

    cand_clean = re.sub(r',(\s*[}\]])', r'\1', cand)
    try:
        obj = json.loads(cand_clean)
    except Exception as e:
        try:
            obj = json.loads(cand)
        except Exception:
            raise RuntimeError(f"Claude JSON è§£æå¤±è´¥ï¼š{e}. åŸå§‹è¿”å›ç‰‡æ®µï¼š{text[:200]}")

    if not isinstance(obj, dict):
        raise RuntimeError("Claude è¿”å›çš„ä¸æ˜¯ JSON å¯¹è±¡ã€‚")
    return {str(k): str(v) for k, v in obj.items()}

def translate_values_batched(
    values: List[str],
    target_lang: str,
    what: str,
    model: str,
    never_translate: Optional[List[str]] = None,
    batch_size: int = 100,
    sleep_sec: float = 0.35,
) -> Dict[str, str]:
    """
    å°† values åˆ†æ‰¹é€ç»™ claude_translateï¼Œåˆå¹¶ç»“æœï¼Œé¿å…é•¿è¾“å‡ºè¢«æˆªæ–­ã€‚
    å¯¹æ¯ä¸ª batch åŠ æŒ‡æ•°é€€é¿é‡è¯•ï¼ŒæŠ— 5xx/ç½‘ç»œæŠ–åŠ¨ã€‚
    """
    all_map: Dict[str, str] = {}
    unique_vals = list(dict.fromkeys(values))  # å»é‡ä¸”ä¿åº

    for idx, chunk in enumerate(_chunk_list(unique_vals, batch_size), start=1):
        # å¯¹å•ä¸ª batch åšæœ€å¤š 4 æ¬¡é‡è¯•
        last_err = None
        for attempt in range(4):
            try:
                part = claude_translate(chunk, target_lang, what, model, never_translate=never_translate)
                # å…œåº•ï¼šç¡®ä¿æ¯ä¸ªè¾“å…¥éƒ½æœ‰æ˜ å°„ã€‚è‹¥ç¼ºå¤±åˆ™å›é€€ä¸ºåŸæ–‡
                for src in chunk:
                    all_map[src] = part.get(src, src)
                break
            except Exception as e:
                last_err = e
                if attempt < 3:
                    delay = (0.8 * (2 ** attempt)) + random.uniform(0, 0.5)
                    print(f"[warn] Batch {idx} attempt {attempt+1} failed: {e}; retry in {delay:.2f}s")
                    time.sleep(delay)
                else:
                    raise RuntimeError(f"Batch {idx} translate failed: {e}") from e

        if sleep_sec:
            time.sleep(sleep_sec)

    return all_map

# ========== æ–‡æœ¬å¤„ç† ==========
def ensure_trailing_slash(s:str)->str:
    return s if (not s or s.endswith("/")) else s+"/"

def split_path_id(val:str)->Tuple[str,str]:
    if "/" in val:
        a,b=val.rsplit("/",1); return a,b
    return "",val

def transform_doc_id(val:str, folder_prefix:str, file_prefix:str)->str:
    if not val or val.startswith(("http://","https://","#")): return val
    folder_prefix=ensure_trailing_slash(folder_prefix)
    base=val[len(folder_prefix):] if folder_prefix and val.startswith(folder_prefix) else val
    parent,last=split_path_id(base)
    if not last.startswith(file_prefix): last=f"{file_prefix}{last}"
    new_path=(parent+"/" if parent else "")+last
    return (folder_prefix+new_path) if folder_prefix else new_path

# ========== è§£æ sidebars.jsï¼šæ”¯æŒå˜é‡å¯¼å‡º ==========
def _skip_string(js: str, i: int) -> int:
    q = js[i]; i += 1
    while i < len(js):
        ch = js[i]
        if ch == '\\': i += 2; continue
        if ch == q: return i
        i += 1
    return i

def _chunk_list(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

def _match_span(js: str, lpos: int, lchar: str, rchar: str) -> Optional[Tuple[int, int]]:
    depth = 0; i = lpos
    while i < len(js):
        ch = js[i]
        if ch in ("'", '"'):
            i = _skip_string(js, i)
        elif ch == lchar:
            depth += 1
        elif ch == rchar:
            depth -= 1
            if depth == 0:
                return (lpos, i)
        i += 1
    return None

def find_top_object_span(js_text: str) -> Optional[Tuple[int, int]]:
    """
    æ”¯æŒï¼š
      a) export default { ... }
      b) module.exports = { ... }
      c) export default IDENT / module.exports = IDENT   +   const/let/var IDENT = { ... }
    è¿”å›å¯¹è±¡å­—é¢é‡èŠ±æ‹¬å·çš„èµ·æ­¢ä½ç½®ï¼ˆå«èŠ±æ‹¬å·ï¼‰ã€‚
    """
    # a/bï¼šç›´æ¥å¯¼å‡ºå¯¹è±¡å­—é¢é‡
    m = re.search(r'(?:export\s+default|module\.exports\s*=)\s*(\{)', js_text)
    if m:
        brace_l = m.start(1)
        return _match_span(js_text, brace_l, '{', '}')

    # cï¼šå¯¼å‡ºå˜é‡å
    m = re.search(r'(?:export\s+default|module\.exports\s*=)\s*([A-Za-z_$][\w$]*)', js_text)
    if not m:
        return None
    ident = m.group(1)

    # åå‘æŸ¥æ‰¾è¯¥å˜é‡çš„å¯¹è±¡å®šä¹‰ï¼šconst/let/var IDENT = { ... }
    pat = re.compile(rf'(?:const|let|var)\s+{re.escape(ident)}\s*=\s*(\{{)', re.MULTILINE)
    m2 = pat.search(js_text)
    if not m2:
        return None
    brace_l = m2.start(1)
    return _match_span(js_text, brace_l, '{', '}')

def find_key_array_span(js_text: str, key: str) -> Optional[Tuple[int, int]]:
    obj_span = find_top_object_span(js_text)
    if not obj_span:
        return None
    obj_l, obj_r = obj_span
    body = js_text[obj_l:obj_r+1]

    # æ”¯æŒ "ProductSidebar": [ ... ] / 'ProductSidebar': [ ... ] / ProductSidebar: [ ... ]
    key_pat = re.compile(
        rf'(?:"{re.escape(key)}"|\'{re.escape(key)}\'|\b{re.escape(key)}\b)\s*:\s*(\[)', 
        re.MULTILINE
    )
    for m in key_pat.finditer(body):
        arr_l = obj_l + m.start(1)
        span = _match_span(js_text, arr_l, '[', ']')
        if span:
            return span
    return None

def extract_array_text(js_text: str, key: str) -> Tuple[str, Tuple[int, int]]:
    span = find_key_array_span(js_text, key)
    if not span:
        raise RuntimeError(f"æ‰¾ä¸åˆ° {key}: [ ... ]")
    a, b = span
    return js_text[a:b+1], span

def upsert_key_array(js_text: str, key: str, new_array_text: str) -> str:
    # è‹¥å·²å­˜åœ¨ â†’ åªæ›¿æ¢æ–¹æ‹¬å·åŒºé—´
    span = find_key_array_span(js_text, key)
    if span:
        a, b = span
        return js_text[:a] + new_array_text + js_text[b+1:]

    # å¦åˆ™æ’å…¥åˆ°é¡¶å±‚å¯¹è±¡ç»“æŸå¤§æ‹¬å·å‰
    obj_span = find_top_object_span(js_text)
    if not obj_span:
        raise RuntimeError("æœªè¯†åˆ«åˆ°é¡¶å±‚å¯¼å‡ºçš„å¯¹è±¡ï¼ˆexport default/module.exportsï¼‰")
    obj_l, obj_r = obj_span

    # ä¼°ç®—ç¼©è¿›ï¼šå–ç»“æŸèŠ±æ‹¬å·ä¸Šä¸€è¡Œçš„å‰å¯¼ç©ºç™½
    before = js_text[:obj_r]
    last_nl = before.rfind("\n")
    indent = "  "
    if last_nl != -1:
        line = before[last_nl+1:obj_r]
        m = re.match(r'^\s*', line)
        indent = m.group(0) or "  "

    insert = f"\n{indent}{key}: {new_array_text},"
    return js_text[:obj_r] + insert + js_text[obj_r:]

# ========== æŠ½å–/æ›¿æ¢ ==========
def collect_unique_values(array_text:str, field:str)->List[str]:
    pat=re.compile(rf'\b{re.escape(field)}\s*:\s*([\'"])(.*?)\1',re.DOTALL)
    seen:Set[str]=set(); out:List[str]=[]
    for m in pat.finditer(array_text):
        val=m.group(2)
        if val not in seen:
            seen.add(val); out.append(val)
    return out

def apply_mapping_to_field(array_text:str, field:str, mapping:Dict[str,str])->str:
    pat=re.compile(rf'(\b{re.escape(field)}\s*:\s*)([\'"])(.*?)(\2)',re.DOTALL)
    def _sub(m):
        prefix,quote,val=m.group(1),m.group(2),m.group(3)
        if val in mapping: return f"{prefix}{quote}{mapping[val]}{quote}"
        return m.group(0)
    return pat.sub(_sub,array_text)

def _rewrite_and_format_items_arrays(js_text: str, folder_prefix: str, file_prefix: str) -> str:
    """
    å®‰å…¨æ”¹å†™ items æ•°ç»„ï¼ˆä»»æ„åµŒå¥—å±‚çº§ï¼‰ï¼š
      - é¡¶å±‚å­—ç¬¦ä¸²å…ƒç´ ï¼šæ›¿æ¢ä¸ºåŠ å‰ç¼€åçš„æ–‡æ¡£ id
      - è‹¥é¡¶å±‚æ‰€æœ‰å…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²ï¼šç¾åŒ–ä¸ºä¸€è¡Œä¸€ä¸ªï¼ˆä¿ç•™ items è¡Œç¼©è¿›ï¼‰
      - è‹¥æ··åˆï¼ˆå«å¯¹è±¡/æ•°ç»„/è¡¨è¾¾å¼ç­‰ï¼‰ï¼šä»…æ›¿æ¢å­—ç¬¦ä¸²ï¼Œæ•´ä½“æ’ç‰ˆå®Œå…¨ä¿æŒåŸæ ·
    ï¼ˆæ³¨ï¼šbuild_lang_sidebar() é‡Œå·²å…ˆå‰¥ç¦»æ³¨é‡Šï¼Œè¿™é‡Œæ— éœ€å¤„ç†æ³¨é‡Šï¼‰
    """
    s = js_text
    n = len(s)
    i = 0
    pos = 0        # â† å…³é”®ï¼šè®°å½•â€œå°šæœªè¾“å‡ºâ€çš„èµ·ç‚¹
    out = []

    def process_inner(inner: str):
        # æ‰«æ innerï¼Œæ›¿æ¢é¡¶å±‚å­—ç¬¦ä¸²å­—é¢é‡ï¼ŒåŒæ—¶åˆ¤æ–­æ˜¯å¦â€œå…¨éƒ¨ä¸ºå­—ç¬¦ä¸²å…ƒç´ â€
        out_chars = []
        parts_for_pretty = []  # æ”¶é›†æ›¿æ¢åçš„é¡¶å±‚å­—ç¬¦ä¸²ï¼ˆç”¨äºçº¯å­—ç¬¦ä¸²åœºæ™¯çš„ç¾åŒ–ï¼‰
        j = 0
        m = len(inner)
        depth = 0
        all_top_level_are_strings = True
        seen_any_element = False

        while j < m:
            ch = inner[j]

            # å­—ç¬¦ä¸²å­—é¢é‡
            if ch in ("'", '"'):
                start = j
                end = _skip_string(inner, j)  # è¿”å›å¼•å·ä½ç½®
                lit = inner[start:end+1]      # åŒ…å«å¼•å·
                if depth == 0:
                    seen_any_element = True
                    q = lit[0]
                    val = lit[1:-1]
                    new_val = transform_doc_id(val, folder_prefix, file_prefix)
                    new_lit = f"{q}{new_val}{q}"
                    out_chars.append(new_lit)
                    parts_for_pretty.append(new_lit)
                else:
                    out_chars.append(lit)
                    all_top_level_are_strings = False
                j = end + 1
                continue

            # é¡¶å±‚çš„éç©ºç™½éé€—å·ï¼šå‡ºç°å¯¹è±¡/æ•°ç»„/æ ‡è¯†ç¬¦ç­‰ï¼Œåˆ¤å®šä¸ºæ··åˆ
            if depth == 0 and ch not in " \t\r\n,":
                if ch in "[{(":
                    depth += 1
                out_chars.append(ch)
                all_top_level_are_strings = False
                seen_any_element = True
                j += 1
                continue

            # é¡¶å±‚ç©ºç™½/é€—å·ï¼šç…§æŠ„
            if depth == 0:
                out_chars.append(ch)
                j += 1
                continue

            # éé¡¶å±‚ï¼šè·Ÿè¸ªæ·±åº¦å¹¶ç…§æŠ„
            if ch in "[{(":
                depth += 1
            elif ch in "]})":
                depth -= 1
            out_chars.append(ch)
            j += 1

        return "".join(out_chars), (all_top_level_are_strings and seen_any_element), parts_for_pretty

    while i < n:
        ch = s[i]

        # è·³è¿‡å­—ç¬¦ä¸²
        if ch in ("'", '"'):
            i = _skip_string(s, i) + 1
            continue

        # å°è¯•åŒ¹é… items\s*:\s*\[
        if s.startswith("items", i):
            # ç¡®ä¿æ˜¯æ ‡è¯†ç¬¦è¾¹ç•Œ
            prev = s[i-1] if i > 0 else ""
            nxt = s[i+5] if i+5 < n else ""
            if (not prev.isalnum() and prev not in "_$") and (not nxt.isalnum() and nxt not in "_$"):
                m1 = re.match(r'items\s*:\s*', s[i:])
                if m1:
                    pre_end = i + m1.end()      # æŒ‡å‘ '[' ä¹‹å‰
                    if pre_end < n and s[pre_end] == '[':
                        arr_l = pre_end         # '[' ä½ç½®
                        # æ‰¾åˆ°åŒ¹é…çš„ ']'
                        depth = 1
                        j = arr_l + 1
                        while j < n and depth > 0:
                            cj = s[j]
                            if cj in ("'", '"'):
                                j = _skip_string(s, j) + 1
                                continue
                            if cj == '[':
                                depth += 1
                            elif cj == ']':
                                depth -= 1
                            j += 1
                        if depth == 0:
                            arr_r = j - 1  # ']' ä½ç½®
                            inner = s[arr_l+1:arr_r]
                            replaced_inner, all_strings, pretty_parts = process_inner(inner)

                            # è®¡ç®—ç¼©è¿›
                            line_start = s.rfind('\n', 0, i) + 1
                            leading = re.match(r'\s*', s[line_start:i]).group(0)
                            item_indent = leading + '  '

                            # å…ˆæŠŠâ€œæœªè¾“å‡ºçš„å‰ç¼€â€å†™å‡ºå»
                            out.append(s[pos:i])

                            if not all_strings:
                                # æ··åˆï¼šä¿æŒåŸæ’ç‰ˆï¼Œåªæ›¿æ¢é¡¶å±‚å­—ç¬¦ä¸²
                                # å…³é”®ï¼šå¯¹ inner å†é€’å½’å¤„ç†ï¼Œç¡®ä¿å…¶ä¸­çš„ items ä¹Ÿè¢«æ”¹å†™
                                inner_processed = _rewrite_and_format_items_arrays(replaced_inner, folder_prefix, file_prefix)

                                out.append(s[i:pre_end])         # 'items ... : '
                                out.append('[')
                                out.append(inner_processed)
                                out.append(']')
                            else:
                                # çº¯å­—ç¬¦ä¸²ï¼šå¤šè¡Œç¾åŒ–
                                lines = [f"{s[i:pre_end]}["]
                                for lit in pretty_parts:
                                    lines.append(f"{item_indent}{lit},")
                                lines.append(f"{leading}]")
                                out.append("\n".join(lines))

                            # ç§»åŠ¨æ¸¸æ ‡ï¼šè·³è¿‡æ•´ä¸ª items æ•°ç»„
                            pos = j        #  ] åä¸€ä½
                            i = j
                            continue
        # æ²¡å‘½ä¸­ï¼šç»§ç»­æ‰«æ
        i += 1

    # æ‹¼ä¸Šæœ€åä¸€æ®µæœªè¾“å‡ºæ–‡æœ¬
    out.append(s[pos:])
    return "".join(out)

def rewrite_doc_refs(array_text:str, folder_prefix:str, file_prefix:str)->str:
    # 1) id: '...'
    pat_id = re.compile(r'(\bid\s*:\s*)([\'"])(.*?)(\2)', re.DOTALL)
    def _sid(m):
        prefix, quote, val = m.group(1), m.group(2), m.group(3)
        return f"{prefix}{quote}{transform_doc_id(val, folder_prefix, file_prefix)}{quote}"
    out = pat_id.sub(_sid, array_text)

    # 2) link: { type: 'doc', id: '...' } æˆ– link: {id:'...'}
    pat_link_doc = re.compile(
        r'(link\s*:\s*\{\s*[^{}]*?\b(?:type\s*:\s*[\'"]doc[\'"]\s*,\s*)?id\s*:\s*)([\'"])(.*?)(\2)',
        re.DOTALL
    )
    def _slink(m):
        prefix, quote, val = m.group(1), m.group(2), m.group(3)
        return f"{prefix}{quote}{transform_doc_id(val, folder_prefix, file_prefix)}{quote}"
    out = pat_link_doc.sub(_slink, out)

    # 3) itemsï¼šé€å­—ç¬¦æ‰«æï¼Œä»»æ„åµŒå¥—å±‚çº§
    prev = out
    for _ in range(10):
        curr = _rewrite_and_format_items_arrays(prev, folder_prefix, file_prefix)
        if curr == prev:
            break
        prev = curr
    out = prev

    return out

# ========== ä¸»æµç¨‹ ==========
def build_lang_sidebar(en_array_text:str, *, target_lang:str, translate_dirname:bool,
                       folder_prefix:str, file_prefix:str, model:str)->str:
    out = en_array_text

    # === æ–°å¢ï¼šå…ˆå‰¥ç¦»æ³¨é‡Šï¼Œæ‰€æœ‰æ”¹å†™å¯¹å»æ³¨é‡Šæ–‡æœ¬è¿›è¡Œ ===
    work, cmt_map = _strip_comments_preserve_tokens(out)

    # 1) ç¿»è¯‘ labelï¼ˆå¯¹å»æ³¨é‡Šæ–‡æœ¬æ”¶é›†/æ›¿æ¢ï¼‰
    labels = collect_unique_values(work, "label")
    if labels:
        print(f"[i] labels: {len(labels)}; batch={int(os.environ.get('CLAUDE_BATCH_SIZE','100'))}")
        label_map = translate_values_batched(
            labels,
            target_lang,
            "label",
            model,
            never_translate=None,
            batch_size=int(os.environ.get("CLAUDE_BATCH_SIZE", "100")),
        )
        work = apply_mapping_to_field(work, "label", label_map)
        time.sleep(0.35)

    # 2) dirNameï¼šä¸ç¿»è¯‘ï¼ŒåªåŠ å‰ç¼€ï¼ˆå¯¹å»æ³¨é‡Šæ–‡æœ¬ï¼‰
    if translate_dirname:
        # è¿™é‡Œä½ çš„é¢„è®¾æœ¬å°± Falseï¼›è‹¥æœ‰äººæ”¹æˆ Trueï¼Œæˆ‘ä»¬ä¹Ÿå»ºè®®ä¸ç¿»è¯‘ï¼Œä»…åŠ å‰ç¼€ï¼š
        fp = ensure_trailing_slash(folder_prefix)
        if fp:
            pat = re.compile(r'(\bdirName\s*:\s*)([\'"])(.*?)(\2)', re.DOTALL)
            def _sub(m):
                prefix, quote, val = m.group(1), m.group(2), m.group(3)
                if not val.startswith(fp):
                    val = fp + val.lstrip('/')
                return f"{prefix}{quote}{val}{quote}"
            work = pat.sub(_sub, work)
    else:
        fp = ensure_trailing_slash(folder_prefix)
        if fp:
            pat = re.compile(r'(\bdirName\s*:\s*)([\'"])(.*?)(\2)', re.DOTALL)
            def _sub(m):
                prefix, quote, val = m.group(1), m.group(2), m.group(3)
                if not val.startswith(fp):
                    val = fp + val.lstrip('/')
                return f"{prefix}{quote}{val}{quote}"
            work = pat.sub(_sub, work)

    # 3) æ”¹å†™é“¾æ¥ + items å¤šè¡Œé‡æ’ï¼ˆå¯¹å»æ³¨é‡Šæ–‡æœ¬ï¼‰
    work = rewrite_doc_refs(work, folder_prefix, file_prefix)

    # === æ–°å¢ï¼šæŠŠæ³¨é‡Šè¿˜åŸå›å»ï¼ˆæ³¨é‡Šå†…å®¹å®Œå…¨ä¸å—å½±å“ï¼‰
    out = _restore_comments_from_tokens(work, cmt_map)
    return out

def run_once(sidebars_path:Path, product_key:str, lang:str, model:str, dry_run:bool=False):
    # è¯» .env
    load_env()

    # è¯­è¨€é¢„è®¾
    preset=LANG_PRESETS.get(lang)
    if not preset:
        raise SystemExit(f"ä¸æ”¯æŒçš„è¯­è¨€ï¼š{lang}ï¼ˆå¯é€‰ {', '.join(LANG_PRESETS.keys())}ï¼‰")
    target_key=preset["target_key"]
    folder=preset["folder"]
    fileprefix=preset["fileprefix"]
    translate_dirname=preset["translate_dirname"]

    js=sidebars_path.read_text(encoding="utf-8")

    # å–è‹±æ–‡æºæ•°ç»„
    en_array, _ = extract_array_text(js, product_key)

    # æ„å»ºç›®æ ‡æ•°ç»„
    out_arr=build_lang_sidebar(
        en_array,
        target_lang=lang,
        translate_dirname=translate_dirname,
        folder_prefix=folder,
        file_prefix=fileprefix,
        model=model or ANTHROPIC_MODEL_DEFAULT
    )

    # ç”Ÿæˆ/è¦†ç›–ç›®æ ‡é”®
    out_js=upsert_key_array(js,target_key,out_arr)

    # æ‘˜è¦
    print(f"\n=== è®¡åˆ’æ›´æ–° ===")
    print(f"è¯­è¨€: {lang}  â†’  ç›®æ ‡é”®: {target_key}")
    print(f"ç›®å½•å‰ç¼€: {folder}   æ–‡ä»¶å‰ç¼€: {fileprefix}")
    print(f"ç¿»è¯‘ dirName: {translate_dirname}")
    print(f"æ¨¡å‹: {model or ANTHROPIC_MODEL_DEFAULT}")
    if dry_run:
        print("\n--dry-run æ¨¡å¼ï¼Œä¸å†™ç›˜ã€‚")
        return

    # å¤‡ä»½å¹¶å†™ç›˜
    backup=sidebars_path.with_suffix(sidebars_path.suffix+".bak")
    backup.write_text(js,encoding="utf-8")
    sidebars_path.write_text(out_js,encoding="utf-8")
    print(f"\nâœ… å·²æ›´æ–°ï¼š{sidebars_path}")
    print(f"ğŸ—‚ å¤‡ä»½ï¼š{backup}")

# ========== CLI ==========
def main():
    parser=argparse.ArgumentParser(description="ä» ProductSidebar ç”Ÿæˆå¤šè¯­è¨€ sidebarï¼ˆåªé€‰è¯­è¨€å³å¯ï¼‰")
    parser.add_argument("--lang", required=True, help=f"ç›®æ ‡è¯­è¨€ï¼š{', '.join(LANG_PRESETS.keys())}")
    parser.add_argument("--sidebars", default="sidebars.js", help="sidebars.js è·¯å¾„ï¼ˆé»˜è®¤ ./sidebars.jsï¼‰")
    parser.add_argument("--product-key", default="ProductSidebar", help="è‹±æ–‡æºé”®åï¼ˆé»˜è®¤ ProductSidebarï¼‰")
    parser.add_argument("--model", default=ANTHROPIC_MODEL_DEFAULT, help="Claude æ¨¡å‹å")
    parser.add_argument("--dry-run", action="store_true", help="åªæ‰“å°å˜æ›´æ‘˜è¦ï¼Œä¸å†™ç›˜")
    args=parser.parse_args()

    sidebars_path=Path(args.sidebars)
    if not sidebars_path.exists():
        raise SystemExit(f"æ‰¾ä¸åˆ° {sidebars_path}ï¼Œè¯·ç”¨ --sidebars æŒ‡å®šæ­£ç¡®è·¯å¾„ã€‚")

    run_once(
        sidebars_path=sidebars_path,
        product_key=args.product_key,
        lang=args.lang,
        model=args.model,
        dry_run=args.dry_run
    )

if __name__=="__main__":
    main()
